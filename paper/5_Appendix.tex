%%% Appendix %%%

\section{Auxiliary Lemmas and Facts}

    In this section we list auxiliary facts and our results that we use several times in our proofs.

    \subsection{Squared norm of the sum}
    \label{axil:squared}
        For all $x_1, ... , x_n \in \mathbb{R}^n$, where $n \in \{2, 4\}$

        \begin{equation*}
            \norms{x_1 + x_2 + ... + x_n}^2 \leq n \norms{x_1}^2 + ... + n \norms{x_n}^2
        \end{equation*}

    \subsection{Cauchyâ€“Schwarz inequality}
    \label{axil:cauchy_schwarz}
        For all $x, y \in \mathbb{R}^d$

        \begin{equation*}
            \dotprod{x}{y} \leq \norms{x}\norms{y}
        \end{equation*}

    \subsection{Fenchel-Young inequality}
    \label{axil:fenchel_young}
    For all $x, y \in \mathbb{R}^d$ and $\beta > 0$
    
    \begin{equation*}
        2 \dotprod{x}{y} \leq \beta^{-1} \|x\|^2 + \beta \|y\|^2 .
    \end{equation*}

    \subsection{Recursion Lemma}
    \begin{lemma}
    \label{lem:baskirskaya_lemma}
        For all $x \in [0; 1)$ consider a function 
        $$\phi(x) := 1 - (1 - x)^{\alpha} - \max\{1, \alpha\} x.$$
        
        Then for all $0 \leq x < 1$ and $\alpha \in \mathbb{R}$ we can obtain that  $\phi(x) \leq 0$.
    \end{lemma}
    \begin{proof}
        First consider the case of $\alpha \notin (0; 1)$. Then we can write out Bernoulli's inequality: for all  $x < 1$ it holds that

        \begin{equation*}
            (1 - x)^{\alpha} \geq 1 - \alpha x.
        \end{equation*}

        Therefore for $0 \leq x < 1$:

        \begin{equation*}
            \phi(x) = 1 - (1 - x)^{\alpha} - \max\{1, \alpha\} x \leq 1 - (1 - x)^{\alpha} - \alpha x \leq 0.
        \end{equation*}

        Now we consider case $0 < \alpha < 1$, therefore $\phi(x)$ takes the form

        \begin{equation*}
            \phi(x) = 1 - (1 - x)^{\alpha} -  x.
        \end{equation*}

        Note that

        \begin{equation*}
            \phi''(x) = \alpha(1 - \alpha)(1 - x)^{\alpha-2} > 0.
        \end{equation*}

        Therefore $\phi(x)$ is convex on a segment $[0; 1]$ and $\psi(0) = \psi(1) = 0$, that means that $\phi(x) \leq 0$ for all $x \in [0; 1)$. This finishes the proof.
    \end{proof}

    \begin{lemma}[Recursion Lemma]
    \label{lem:recursion}
    Suppose we have the following recurrence relation for variables $\{r_k\}_{k=0}^N \subset \mathbb{R}$

    \begin{equation}
    \label{eq:recur_start}
        r_{k+1} \leq \left(1 - \frac{\beta_0}{(k + k_0)^{\alpha_0}}\right) r_{k} + \sum\limits_{i = 1}^m \frac{\beta_i}{(k + k_0)^{\alpha_i}},
    \end{equation}

    where $\beta_i > 0 ~~ \forall i \in \overline{0, m}$, $0 \leq \alpha_0 \leq 1$, $\alpha_i \in \mathbb{R} ~~ \forall i \in \overline{1, m}$. 
    
    Then we can estimate the convergence of the sequence $\{r_k\}_{k=0}^N$ to zero:

    \begin{equation}
    \label{eq:recur_end}
        r_k \leq 2 \cdot \sum\limits_{i = 1}^m \frac{Q_i}{(k+k_0)^{\alpha_i - \alpha_0}},
    \end{equation}

    where $Q_{i^*} = \max\{\beta_{i^*} / \beta_0, r_0 k_0^{\alpha_{i^*} - \alpha_0}\}$ and $Q_i = \beta_i / \beta_0$ if $i \neq i^*$, where $i^*$ we can choose arbitrarily from the set $\overline{1, m}$, and

    \begin{enumerate}
        \item[$\bullet$] if $~0 \leq \alpha_0 < 1$:

        \begin{equation*}
            k_0 \geq \left( \frac{2}{\beta_0} \max\{1, \max\{\alpha_i\} - \alpha_0\} \right)^{\frac{1}{1 - \alpha_0}} ~~ \text{ and } ~~ \beta_0 > 0.
        \end{equation*}

        \item[$\bullet$] if $~\alpha_0 = 1$:

        \begin{equation*}
            k_0 \in \mathbb{N} ~~\text{ and }~~ \beta_0 \geq 2 \max\{1, \max\{\alpha_i\} - 1\}.
        \end{equation*}
    \end{enumerate}
        
    \end{lemma}
    \begin{proof}
        We prove the claim in \eqref{eq:recur_end} by induction. First, note that

        \begin{equation*}
            r_0 = r_0 \cdot \left(\frac{k_0}{0 + k_0}\right)^{\alpha_{i^*} - \alpha_0} 
            \leq 
            \frac{Q_{i^*}}{(0 + k_0)^{\alpha_{i^*} - \alpha_0}}
            \leq
            2 \cdot \sum\limits_{i = 0}^m \frac{Q_i}{(0 + k_0)^{\alpha_i - \alpha_0}}.
        \end{equation*}
        
        and therefore the base step of the induction holds true. 
        
        Now assume that the condition in \eqref{eq:recur_end} holds for some $k$. Now we will show that this condition will hold for $k + 1$.

        We start by fitting \eqref{eq:recur_end} into the original recurrence relation \eqref{eq:recur_start} and using that $\beta_i \leq Q_i \beta_0$:

        \begin{equation*}
        \begin{split}
            r_{k+1} &\leq
            \left(1 - \frac{\beta_0}{(k + k_0)^{\alpha_0}}\right) \cdot \left(2 \sum\limits_{i = 1}^m \frac{Q_i}{(k + k_0)^{\alpha_i - \alpha_0}} \right) + \sum\limits_{i=1}^m \frac{\beta_i}{(k + k_0)^{\alpha_i}}
            \\&\leq
            2 \sum\limits_{i = 1}^m \frac{Q_i}{(k + k_0)^{\alpha_i - \alpha_0}}
            -
            \sum\limits_{i = 1}^m \frac{Q_i \beta_0}{(k + k_0)^{\alpha_i}}
            =
            \sum\limits_{i = 1}^m \left(\frac{2 Q_i}{(k + k_0)^{\alpha_i - \alpha_0}} - \frac{Q_i \beta_0}{(k + k_0)^{\alpha_i}} \right).
        \end{split}
        \end{equation*}

        Our goal is to show that for all $i \in \overline{1, m}$ it holds that 

        \begin{equation}
        \label{eq:desired_ineq}
            \frac{2 Q_i}{(k + k_0)^{\alpha_i - \alpha_0}} - \frac{Q_i \beta_0}{(k + k_0)^{\alpha_i}} \leq \frac{2 Q_i}{(k + k_0 + 1)^{\alpha_i - \alpha_0}}.
        \end{equation}

        Let us rewrite this inequality in such a way that it takes a more convenient form:

        \begin{equation*}
            \frac{2}{\beta_0}\underbrace{ \left[  1 - \left( 1 - \frac{1}{k + k_0 + 1}\right)^{\alpha_i - \alpha_0} \right]}_{\circledOne} \leq \left( \frac{1}{k+k_0}\right)^{\alpha_0}.
        \end{equation*}

        Using Lemma \ref{lem:baskirskaya_lemma} with $x = (k + k_0 + 1)^{-1} \in [0; 1)$ and $\alpha = \alpha_i - \alpha_0$ we can obtain that

        \begin{equation*}
            \circledOne \leq \max\{1, \alpha_i - \alpha_0\} \frac{1}{k+k_0 + 1} \leq \max\{1, \alpha_i - \alpha_0\} \frac{1}{k + k_0}.
        \end{equation*}

        Now our desired inequality \eqref{eq:desired_ineq} takes form 

        \begin{equation*}
            \frac{2}{\beta_0} \max\{1, \alpha_i - \alpha_0\} \frac{1}{k + k_0} \leq \left( \frac{1}{k+k_0}\right)^{\alpha_0}.
        \end{equation*}

        Again, we rewrite it in a more convenient form:

        \begin{equation}
        \label{eq:desired_ineq_final}
            \frac{2}{\beta_0} \max\{1, \alpha_i - \alpha_0\} \leq (k + k_0)^{1 - \alpha_0}.
        \end{equation}

        Now consider two cases

        \begin{enumerate}
            \item[$\bullet$] If $~0 \leq \alpha_0 < 1$.

            In this case $(k + k_0)^{1 - \alpha_0} \geq k_0^{1 - \alpha_0}$ and if we take

            \begin{equation*}
                k_0 \geq \left( \frac{2}{\beta_0} \max\{1, \max\{\alpha_i\} - \alpha_0\} \right)^{\frac{1}{1 - \alpha_0}},
            \end{equation*}

            then according to \eqref{eq:desired_ineq_final} desired inequality \eqref{eq:desired_ineq} will be fulfilled for all $i \in \overline{1, m}$ for all $\beta_0 > 0$. 

            \item[$\bullet$] If $~\alpha_0 = 1$, then inequality \eqref{eq:desired_ineq_final} takes form

            \begin{equation*}
                \frac{2}{\beta_0} \max\{1, \alpha_i - 1\} \leq 1.
            \end{equation*}

            Therefore if we take

            \begin{equation*}
                \beta_0 \geq 2 \max\{1, \max\{\alpha_i\} - 1\},
            \end{equation*}

            then again according to \eqref{eq:desired_ineq_final} desired inequality \eqref{eq:desired_ineq} will be fulfilled for all $i \in \overline{1, m}$ for all $k_0 \in \mathbb{N}$.
        \end{enumerate}

        This finishes the proof.
        
    \end{proof}

    \section{Proof of converge rate of JAGUAR Algorithm \ref{alg:JAGUAR}. Non-stochastic case.} \label{appendix:JAGUAR_nonstoch}

        \begin{proof}[Proof of Theorem \ref{theorem:JAGUAR_nonstoch}]
            We start by writing out result from Lemma \ref{lemma:h_vs_nablaf_nonstoch} and setting up $\gamma_k = \frac{4}{k + k_0}$:

            \begin{equation*}
            \begin{split}
                \expect{\norms{h^{k+1} - \nabla f(x^{k+1})}^2}
                &\leq
                \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
                + \frac{32 d L^2 D^2}{(k + k_0)^2} 
                \\&\quad+ L^2 \gamma^2 
                + \frac{2 \Delta^2}{\gamma^2}
            \end{split}
            \end{equation*}
    
            Now we use Lemma \ref{lem:recursion} with $\alpha_0 = 0, \beta_0 = 1/2d$;
            $\alpha_1 = 2, \beta_1 = 32d L^2 D^2$;
            $\alpha_2 = 0, \beta_2 = L^2 \gamma^2 + \frac{2 \Delta^2}{\gamma^2}$ and $i^* = 1$.
    
            \begin{equation*}
                \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = 
                \mathcal{O} \left( d L^2 \gamma^2 
                + \frac{d \Delta^2}{\gamma^2}
                +\frac{\max\{d^2 L^2 D^2, \norms{h^0 - \nabla f(x^0)}^2 \cdot k_0^2\}}{(k + k_0)^2} \right),
            \end{equation*}
    
            where $k_0 = (4d \cdot 2)^1 = 8d$. If $h_0 = \widetilde{\nabla} f_\delta(x^0)$ we can obtain
    
            \begin{equation*}
                \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = 
                \mathcal{O} \left( d L^2 \gamma^2 
                + \frac{d \Delta^2}{\gamma^2}
                +\frac{d^2 L^2 D^2}{(k + 8d)^2} \right)
            \end{equation*}

            This finishes the proof.
        \end{proof}
        
    \section{Proof of converge rate of JAGUAR Algorithm \ref{alg:JAGUAR}. Stochastic case.} \label{appendix:JAGUAR}


    \begin{proof}[proof of Lemma \ref{lemma:tilde_vs_notilda}]
        Let's start by writing out a definition of gradient approximation \eqref{eq:opf_d}:
        
        \begin{equation*}
        \begin{split}
            \expect{\norms{\widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-) - \nabla f(x)}^2}
            &= 
            \expect{\norms{\sum\limits_{i = 1}^d \frac{f_{\delta}(x + \gamma e_i, \xi^+_i) - f_{\delta}(x - \gamma e_i, \xi^-_i)}{2 \gamma}e_i - \nabla f(x)}^2}
            \\&= 
            \expect{\norms{\sum\limits_{i = 1}^d \left(\frac{f_{\delta}(x + \gamma e_i, \xi^+_i) - f_{\delta}(x - \gamma e_i, \xi^-_i)}{2 \gamma} - \dotprod{\nabla f(x)}{e_i} \right) e_i}^2}
            \\&\overset{(\star)}{=}
            \sum\limits_{i = 1}^d \expect{\norms{\left(\frac{f_{\delta}(x + \gamma e_i, \xi^+_i) - f_{\delta}(x - \gamma e_i, \xi^-_i)}{2 \gamma} - \dotprod{\nabla f(x)}{e_i} \right) e_i}^2}
            \\&=
            \sum\limits_{i = 1}^d \expect{\left|\frac{f_{\delta}(x + \gamma e_i, \xi^+_i) - f_{\delta}(x - \gamma e_i, \xi^-_i)}{2 \gamma} - \dotprod{\nabla f(x)}{e_i} \right|^2}
    \end{split}
    \end{equation*}

    The $(\star)$ equality holds since $\dotprod{e_i}{e_j} = 0$ if $i \neq j$. Now let's estimate the value under the summation:

    \begin{equation*}
    \begin{split}
        \expect{\left|\frac{f_{\delta}(x + \gamma e_i, \xi^+_i) - f_{\delta}(x - \gamma e_i, \xi^-_i)}{2 \gamma} - \dotprod{\nabla f(x)}{e_i} \right|^2} 
        &= 
        \mathbb{E}\Bigg[ \Bigg|\frac{f(x + \gamma e_i, \xi^+_i) - f(x - \gamma e_i, \xi^-_i)}{2 \gamma} - \dotprod{\nabla f(x)}{e_i}
        \\&\qquad\quad+ 
        \frac{\delta(x + \gamma e_i, \xi^+_i) - \delta(x - \gamma e_i, \xi^-_i)}{2 \gamma}\Bigg|^2 \Bigg]
        \\&\overset{\ref{axil:squared}}{\leq}
        \frac{1}{2 \gamma^2} \underbrace{\expect{\left|f(x + \gamma e_i, \xi^+_i) - f(x - \gamma e_i, \xi^-_i) - \dotprod{\nabla f(x)}{2 \gamma e_i} \right|^2}}_{\circledOne}
        \\&\qquad\quad+ 
        \frac{2 \Delta^2}{\gamma^2}
    \end{split}
    \end{equation*}

        Last inequality holds since noise is bounded . Consider $\circledOne$. Using \ref{axil:squared} with $n = 4$ we get:

        \begin{equation}
        \label{eq:tmp_lemma_zlp}
        \begin{split}
            \expect{\left| f(x + \gamma e_i, \xi^+_i) - f(x - \gamma e_i, \xi^-_i) - \dotprod{\nabla f(x)}{2 \gamma e_i} \right|^2}
            &\leq
            4 \expect{\left| f(x + \gamma e_i, \xi^+_i) - f(x, \xi^+_i) - \dotprod{\nabla f(x, \xi^+_i)}{ \gamma e_i} \right|^2}
            \\&+
            4 \expect{\left|- f(x - \gamma e_i, \xi^-_i) + f(x, \xi^-_i) + \dotprod{\nabla f(x, \xi^-_i)}{-\gamma e_i} \right|^2}
            \\&+
            4 \expect{\left| f(x, \xi^+_i) - f(x, \xi^-_i) \right|^2}
            \\&+ 
            4 \expect{\left| \dotprod{\nabla f(x, \xi^+_i) + \nabla f(x, \xi^-_i) - 2 \nabla f(x)}{\gamma e_i} \right|^2}
        \end{split}
        \end{equation}

        Let's evaluate all these four components separately. Since functions $f(x, \xi^+_i)$ and $f(x, \xi^-_i)$ are $L(\xi^{\pm}_i)$-smooth we have estimates for first and second:
        \begin{equation}
        \label{eq:tmp_first_and_second_zlp}
        \begin{split}
            &\left| f(x + \gamma e_i, \xi^+_i) - f(x, \xi^+_i) - \dotprod{\nabla f(x, \xi^+_i)}{ \gamma e_i} \right| \leq \frac{L(\xi_i^+)}{2} \gamma^2 \leq \frac{L}{2} \gamma^2
            \\&\left|- f(x - \gamma e_i, \xi^-_i) + f(x, \xi^-_i) + \dotprod{\nabla f(x, \xi^-_i)}{-\gamma e_i} \right| \leq \frac{L(\xi_i^-)}{2} \gamma^2 \leq \frac{L}{2} \gamma^2
        \end{split}
        \end{equation}

        If we consider tpf approximation \eqref{eq:tpf}, then third term in \eqref{eq:tmp_lemma_zlp} equals to zero, since $\xi^+_i = \xi^-_i$, if we consider opf case \eqref{eq:opf}, then we can obtain

        \begin{equation}
        \label{eq:tmp_third_zlp}
            \expect{\left| f(x, \xi^+_i) - f(x, \xi^-_i) \right|^2}
            \leq 2\expect{\left| f(x, \xi^+_i) - f(x) \right|^2} + 2\expect{\left| f(x, \xi^-_i) - f(x) \right|^2} \leq 4 \sigma_f^2
        \end{equation}

        Consider the last point in \eqref{eq:tmp_lemma_zlp} and using Cauchyâ€“Schwarz inequality \ref{axil:cauchy_schwarz} we can obtain:

        \begin{equation}
        \label{eq:tmp_four_zlp}
            \expect{\left| \dotprod{\nabla f(x, \xi^+_i) - \nabla f(x)}{\gamma e_i} \right|^2}
            \leq
            \expect{\norms{\nabla f(x, \xi^+_i) - \nabla f(x)}^2 \gamma^2}
            \leq \sigma_{\nabla}^2 \gamma^2
        \end{equation}

        Combining \eqref{eq:tmp_first_and_second_zlp}, \eqref{eq:tmp_third_zlp} and \eqref{eq:tmp_four_zlp} we obtain

        \begin{equation*}
            \expect{\norms{\widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-) - \nabla f(x)}^2} 
            \leq d L^2 \gamma^2 
            + \frac{8 d \sigma_f^2}{\gamma^2} 
            + 2 d \sigma_{\nabla}^2 + \frac{2 d \Delta^2}{\gamma^2}
        \end{equation*}

        In two point feedback \eqref{eq:tpf} $\sigma_f^2 = 0$.
    \end{proof}


    \begin{proof}[proof of Lemma \ref{lemma:h_vs_nablaf}]
        Let's start by writing out a definition of $h^{k}$ using line \ref{line:h^k} of Algorithm \ref{alg:JAGUAR}

        \begin{equation*}
        \begin{split}
            \expect{\norms{h^{k} - \nabla f(x^{k})}^2} 
            &=
            \expect{\norms{h^{k-1} + \widetilde{\nabla}_i f_{\delta}(x^k, \xi^+, \xi^-) - \dotprod{h^{k-1}}{e_i} e_i - \nabla f(x^{k})}^2}
            \\&=
            \mathbb{E}\Bigg[\Bigg\|
            \left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right) 
            \\&\qquad\quad+ e_i e_i^T \left(\widetilde{\nabla} f_{\delta}(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k) \right)
            \\&\qquad\quad- \left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)\Bigg\|^2\Bigg]
            \\&=
            \underbrace{\expect{\norms{\left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right) }^2}}_{\circledOne}
            \\&\quad+ \underbrace{\expect{\norms{e_i e_i^T \left(\widetilde{\nabla} f_{\delta}(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k) \right)}^2}}_{\circledTwo}
            \\&\quad+
            \underbrace{\expect{\norms{\left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)}^2}}_{\circledThree}
            \\&\quad+
            \underbrace{\expect{2\dotprod{\left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right)}{\left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)}^2}}_{\circledFour}
        \end{split}
        \end{equation*}

        In the last equality the two remaining scalar products are zero, since  $e_i e_i^T \left(I - e_i e_i^T\right) = e_i^T e_i - e_i^T e_i = 0$. Consider the $\circledOne$. Using notation $v := h^{k-1} - \nabla f(x^{k-1})$

        \begin{equation*}
        \begin{split}
            \expect{\norms{\left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right)}^2}
            &=
            \expect{v^T\left(I - e_i e_i^T\right)^T \left(I - e_i e_i^T\right) v}
            \\&=
            \expect{v^T\left(I - e_i e_i^T\right) v}
            = \expect{\mathbb{E}_{k-1}\left[ v^T\left(I - e_i e_i^T\right) v \right]},
        \end{split}
        \end{equation*}

        where $\mathbb{E}_{k-1}[\cdot]$ is the conditional expectation with fixed randomness of all steps up to $k-1$. Since at step $k$ the vectors $e_i$ are generated independently, we obtain

        \begin{equation*}
        \begin{split}
            \expect{\mathbb{E}_{k-1}\left[ v^T\left(I - e_i e_i^T\right) v \right]}
            &=
            \expect{v^T\mathbb{E}_{k-1}\left[\left(I - e_i e_i^T\right) \right] v} = \left(1 - \frac{1}{d}\right) \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}^2}
        \end{split}
        \end{equation*}

        Consider $\circledTwo$. Since we generate $i$ independently, $x^k$ is independent of the $e_i$ generated at step $k$, then we can apply the same technique as in estimation $\circledOne$:

        \begin{equation*}
            \expect{\norms{e_i e_i^T \left(\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k) \right)}^2} 
            =
            \frac{1}{d} \expect{\norms{\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k)}^2}
        \end{equation*}

        Using Lemma \ref{lemma:tilde_vs_notilda} we obtain 

        \begin{equation*}
            \frac{1}{d} \expect{\norms{\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k)}^2}
            \leq
            L^2 \gamma^2 
            + \frac{8 \sigma_f^2}{\gamma^2} 
            + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2}
        \end{equation*}

        Consider $\circledThree$. Using the same technique as in estimation $\circledOne$:

        \begin{equation*}
            \expect{\norms{\left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)}^2} \leq \left(1 - \frac{1}{d}\right) L^2 \expect{\norms{x^k - x^{k-1}}^2}
        \end{equation*}

        Consider $\circledFour$. Using Fenchel-Young inequality \ref{axil:fenchel_young} with $\beta = 2d$ we obtain

        \begin{equation*}
            \circledFour \leq \left(1 - \frac{1}{d}\right) \left(\frac{1}{2d} \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}^2} + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2}\right)
        \end{equation*}

        Therefore it holds that

        \begin{equation*}
        \begin{split}
            \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
            &\leq
            \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}^2}
            + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2}
            \\&\quad+ L^2 \gamma^2 
            + \frac{8 \sigma_f^2}{\gamma^2} 
            + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2}
        \end{split}
        \end{equation*}

    \end{proof}

    \begin{proof}[Proof of Lemma \ref{lemma:rho_vs_nablaf}]
    
        Let's start by writing out a definition of $\rho^{k}$ using line \ref{line:rho^k} of Algorithm \ref{alg:JAGUAR}

        \begin{equation*}
        \begin{split}
            \expect{\norms{\rho^k - \nabla f(x^k)}^2}
            &=
            \expect{\norms{h^{k-1} + d \widetilde{\nabla}_i f_\delta (x^k, \xi^+, \xi^-) - d \dotprod{h^{k-1}}{e_i} e_i - \nabla f(x^k)}^2}
            \\&=
            \mathbb{E}\Bigg[\Bigg\|(I - d e_i e_i^T)\left(h^{k-1} - \nabla f(x^{k-1})\right) 
            \\&\qquad\quad+ 
            de_ie_i^T \left( \widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ... , \xi^+, \xi^-) - \nabla f(x^k)\right)
            \\&\qquad\quad+ 
            (I - d e_i e_i^T)\left(\nabla f(x^{k-1}) - \nabla f(x^k)\right)
            \Bigg\|^2\Bigg]
            \\&\overset{\star}{\leq}
            4 (d-1) \expect{\norms{h^{k-1} - \nabla f(x^{k-1}}} 
            \\&\quad+ 4 d \expect{\norms{\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ... , \xi^+, \xi^-) - \nabla f(x^k)}^2} 
            \\&\quad+ 2 (d-1) \expect{\norms{\nabla f(x^{k-1}) - \nabla f(x^k)}^2}
        \end{split}
        \end{equation*}

        The $\star$ inequality is correct due to similar reasoning as in the proof of Lemma \ref{lemma:h_vs_nablaf} and due to Fenchel-Young inequality \ref{axil:fenchel_young}. Now we can estimate all three summands using Lemmas \ref{lemma:h_vs_nablaf} and \ref{lemma:tilde_vs_notilda} and using Assumption \ref{ass:smooth}:

        \begin{equation*}
        \begin{split}
            \expect{\norms{\rho^k - \nabla f(x^k)}^2}
            &\leq
            4d \expect{\norms{h^{k-1} - \nabla f(x^{k-1}}} 
            \\&+ 4d^2 \left( L^2 \gamma^2 
            + \frac{8 \sigma_f^2}{\gamma^2} 
            + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2} \right)
            + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2}
        \end{split}
        \end{equation*}

        This finishes the proof.
        
    \end{proof}

    \begin{proof}[Proof of Lemma \ref{lemma:g_vs_nabla_f}]
        We start by writing out a definition of $g^k$ using line \ref{line:g^k} of Algorithm \ref{alg:JAGUAR}

        \begin{equation*}
        \begin{split}
            \expect{\norms{g^k - \nabla f(x^k)}^2}
            &=
            \expect{\norms{\nabla f(x^{k-1}) - g^{k-1} + \nabla f(x^k) - \nabla f(x^{k-1}) - \left( g^k - g^{k-1} \right)}^2}
            \\&=
            \expect{\norms{\nabla f(x^{k-1}) - g^{k-1} + \nabla f(x^k) - \nabla f(x^{k-1}) - \eta_k\left( \rho^k - g^{k-1} \right)}^2}
            \\&=
            \expect{\norms{(1-\eta_k)(\nabla f(x^{k-1}) - g^{k-1}) 
            + (1 - \eta_k) (\nabla f(x^k) - \nabla f(x^{k-1})) + \eta_k \left( \nabla f(x^k) - \rho^k \right)}^2}
            \\&=
            (1-\eta_k)^2 \underbrace{\expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}}_{\circledOne} 
            + 
            (1-\eta_k)^2 \underbrace{\expect{\norms{\nabla f(x^k) - \nabla f(x^{k-1})}^2}}_{\circledTwo} 
            \\&\quad+
            \eta_k^2 \underbrace{\expect{\norms{\nabla f(x^k) - \rho^k}^2}}_{\circledThree} 
            +
            2(1 - \eta_k)^2\underbrace{\expect{\dotprod{\nabla f(x^{k-1}) - g^{k-1}}{\nabla f(x^k) - \nabla f(x^{k-1})}}}_{\circledFour}
            \\&\quad+
            2 \eta_k(1 - \eta_k)\underbrace{\expect{\dotprod{\nabla f(x^{k-1}) - g^{k-1}}{\nabla f(x^k) - \rho^k}}}_{\circledFive}
            \\&\quad+
            2 \eta_k(1 - \eta_k)\underbrace{\expect{\dotprod{\nabla f(x^k) - \nabla f(x^{k-1})}{\nabla f(x^k) - \rho^k}}}_{\circledSix}
        \end{split}
        \end{equation*}

        Consider $\circledFive$. Since we generate $\xi^+$ and $\xi^-$ independently, we obtain 

        \begin{equation*}
            \circledFive = \expect{\dotprod{\nabla f(x^{k-1}) - g^{k-1}}{\mathbb{E}_{k - 1} \left[\nabla f(x^k) - \rho^k \right]}},
        \end{equation*}

        where $\mathbb{E}_{k-1}[\cdot]$ is the conditional expectation with fixed randomness of all steps up to $k-1$. Using fact that 

        \begin{equation*}
            \mathbb{E}_{k - 1} \left[\nabla f(x^k) - \rho^k \right] = \nabla f(x^k) - \widetilde{\nabla} f(x^k) = \nabla f(x^k) - \sum\limits_{i = 1}^d \frac{f(x + \gamma e_i) - f(x - \gamma e_i)}{2 \gamma} e_i.
        \end{equation*}

        Fact that for $\widetilde{\nabla} f_\delta(x)$ Lemma \ref{lemma:tilde_vs_notilda} holds true with $\sigma_f^2 = \sigma_\nabla^2 = 0$ and using Cauchy Schwarz inequality \ref{axil:cauchy_schwarz} with $\beta = 2 (1 - \eta_k)$ we can assume

        \begin{equation}
        \label{eq:tmp_th3_5}
            \circledFive \leq \frac{1}{4(1 - \eta_k)} \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2} + (1 - \eta_k) \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2}.
        \end{equation}

        Similarly it can be shown that 

        \begin{equation}
        \label{eq:tmp_th3_6}
            \circledSix \leq \frac{1}{2 (1 - \eta_k) \eta_k^2} \expect{\norms{\nabla f(x^k) - \nabla f(x^{k-1})}^2} 
            +
            \frac{(1 - \eta_k) \eta_k^2}{2} \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2}.
        \end{equation}

        Using Assumption \ref{ass:smooth} we can obtain that 

        \begin{equation}
        \label{eq:tmp_th3_3}
            \circledTwo \leq L^2 \expect{\norms{x^k - x^{k-1}}^2}.
        \end{equation}

        Consider $\circledFour$. Using auchy Schwarz inequality \ref{axil:cauchy_schwarz} with $\beta = 2 \frac{(1 - \eta_k)^2}{\eta_k}$ we can assume

        \begin{equation}
        \label{eq:tmp_th3_4}
            \circledFour \leq \frac{\eta_k}{4 (1 - \eta_k)^2} \expect{\norms{g^k - \nabla f(x^k)}^2}
            +
            \frac{(1 - \eta_k)^2}{\eta_k} L^2 \expect{\norms{x^k - x^{k-1}}^2}
        \end{equation}

        Putting \eqref{eq:tmp_th3_5}, \eqref{eq:tmp_th3_6}, \eqref{eq:tmp_th3_3} and \eqref{eq:tmp_th3_4} all together and using the fact that $(1 -\eta_k)^2 \leq 1 - \eta_k$, we obtain

        \begin{equation*}
        \begin{split}
            \expect{\norms{g^k - \nabla f(x^k)}^2}
            &\leq 
            \left(1 - \eta_k\right) \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}
            +
            \frac{4 L^2}{\eta_k} \expect{\norms{x^k - x^{k-1}}^2}
            \\&\quad+
            \eta_k^2 \expect{\norms{\nabla f(x^k) - \rho^k}^2}
            +
            3 \eta_k \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2}
        \end{split}
        \end{equation*}

        This finishes the proof.
    
    \end{proof}

    \begin{proof}[Proof of Theorem \ref{theorem:JAGUAR}]
        Consider $\expect{\norms{h^{k} - \nabla f(x^{k})}^2}$. We start by writing out result from Lemma \ref{lemma:h_vs_nablaf} and setting up $\gamma_k = \frac{4}{k + k_0}$:

        \begin{equation*}
        \begin{split}
            \expect{\norms{h^{k+1} - \nabla f(x^{k+1})}^2}
            &\leq
            \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
            + \frac{32 d L^2 D^2}{(k + k_0)^2} 
            \\&\quad+ L^2 \gamma^2 
            + \frac{8 \sigma_f^2}{\gamma^2} 
            + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2}
        \end{split}
        \end{equation*}

        Now we use Lemma \ref{lem:recursion} with $\alpha_0 = 0, \beta_0 = 1/2d$;
        $\alpha_1 = 2, \beta_1 = 32d L^2 D^2$;
        $\alpha_2 = 0, \beta_2 = L^2 \gamma^2 + \frac{8 \sigma_f^2}{\gamma^2} + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2}$ and $i^* = 1$.

        \begin{equation*}
            \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = 
            \mathcal{O} \left( d L^2 \gamma^2 
            + \frac{d \sigma_f^2}{\gamma^2} 
            + d \sigma_{\nabla}^2 + \frac{d \Delta^2}{\gamma^2}
            +\frac{\max\{d^2 L^2 D^2, \norms{h^0 - \nabla f(x^0)}^2 \cdot k_0^2\}}{(k + k_0)^2} \right),
        \end{equation*}

        where $k_0 = (4d \cdot 2)^1 = 8d$. For simplicity of calculations further we take $k_0 = 8 d^{3/2} > 8d$. If $h^0 = \widetilde{\nabla} f_\delta(x^0, \xi^+_1, \xi^-_1, ..., \xi^+_d, \xi^-_d)$ we can obtain

        \begin{equation*}
            \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = 
            \mathcal{O} \left( d L^2 \gamma^2 
            + \frac{d \sigma_f^2}{\gamma^2} 
            + d \sigma_{\nabla}^2 + \frac{d \Delta^2}{\gamma^2}
            +\frac{d^2 L^2 D^2}{(k + 8d^{3/2})^2} \right)
        \end{equation*}

        Consider $\expect{\norms{\rho^{k} - \nabla f(x^{k})}^2}$. Using Lemmas \ref{lemma:rho_vs_nablaf} and \ref{lemma:tilde_vs_notilda} we obtain

        \begin{equation*}
            \expect{\norms{\rho^{k} - \nabla f(x^{k})}^2} = 
            \mathcal{O} \left( d^2 L^2 \gamma^2 
            + \frac{d^2 \sigma_f^2}{\gamma^2} 
            + d^2 \sigma_{\nabla}^2 + \frac{d^2 \Delta^2}{\gamma^2}
            +\frac{d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^2} \right)
        \end{equation*}

        Consider $\expect{\norms{g^k - \nabla f(x^k)}^2}$. We write out result from Lemma \ref{lemma:g_vs_nabla_f} and setting up $\eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}}$:

        \begin{equation*}
        \begin{split}
            \expect{\norms{g^k - \nabla f(x^k)}^2}
            &\leq 
            \left(1 - \eta_k\right) \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}
            +
            \frac{4 L^2 D^2}{(k + 8d^{3/2})^{4/3}}
            \\&\quad+
            \frac{4}{(k + 8d^{3/2})^{4/3}} \cdot \mathcal{O} \left(d^2 L^2 \gamma^2 
            + \frac{d^2 \sigma_f^2}{\gamma^2} 
            + d^2 \sigma_{\nabla}^2 + \frac{d^2 \Delta^2}{\gamma^2}
            +\frac{d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^2} \right)
            \\&\quad+
            \frac{12}{(k + 8d^{3/2})^{2/3}} \left( d L^2 \gamma^2  
            + \frac{d \Delta^2}{\gamma^2}\right)
        \end{split}
        \end{equation*}

        Using Lemma \ref{lem:recursion} with $\alpha_0 = 2/3, \beta_0 = 4$; 
        $\alpha_1 = 4/3, \beta_1 = 4 L^2 D^2$; 
        $\alpha_2 = 4/3, \beta_2 = 4 d^2 L^2 \gamma^2 + \frac{4 d^2 \sigma_f^2}{\gamma^2} + 4 d^2 \sigma_{\nabla}^2 + \frac{4 d^2 \Delta^2}{\gamma^2}$;
        $\alpha_3 = 10/3, \beta_3 = 4 d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}$;
        $\alpha_4 = 2/3, \beta_4 = d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2}$ and $i^* = 2$ we get:

        \begin{equation}
        \label{eq:tmp_last_1}
        \begin{split}
            \expect{\norms{g^k - \nabla f(x^k)}^2} 
            &= 
            \mathcal{O} \Bigg(\frac{L^2 D^2 + \max\{d^2 L^2 \gamma^2 + d^2 \sigma_f^2/ \gamma^2 + d^2 \sigma_{\nabla}^2 + d^2 \Delta^2 / \gamma^2, d \norms{g^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{2/3}} 
            \\&\qquad \quad+
            \frac{d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{8/3}} + d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2} \Bigg)
        \end{split}
        \end{equation}

        Since 
        
        $$\frac{d^3 L^2 D^2}{(k + 8d^{3/2})^{8/3}} \leq \frac{L^2 D^2}{(k + 8d^{3/2})^{2/3}} ~~\text{ and }~~
        \frac{d^2 L^2 \gamma^2 + d^2 \Delta^2 / \gamma^2}{(k + 8d^{3/2})^{2/3}} \leq d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2},$$

        we can simplify \eqref{eq:tmp_last_1}:

        \begin{equation*}
        \begin{split}
            \expect{\norms{g^k - \nabla f(x^k)}^2} 
            &=
            \mathcal{O} \Bigg(\frac{L^2 D^2 + \max\{d^2 \sigma_f^2/ \gamma^2 + d^2 \sigma_{\nabla}^2, d \norms{g^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{2/3}}
            \\&\qquad\quad+
            \frac{d^4 \norms{h^0 - \nabla f(x^0)}^2}{(k + 8d^{3/2})^{8/3}}
            +
            d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2} \Bigg)
        \end{split}
        \end{equation*}

        If $h^0 = g^0 = \widetilde{\nabla} f_\delta(x^0, \xi^+_1, \xi^-_1, ..., \xi^+_d, \xi^-_d)$ we can obtain

        \begin{equation*}
            \expect{\norms{g^k - \nabla f(x^k)}^2} 
            =
            \mathcal{O} \left(\frac{L^2 D^2 + d^2 \sigma_f^2/ \gamma^2 + d^2 \sigma_{\nabla}^2}{(k + 8d^{3/2})^{2/3}} 
            +
            d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2} \right)
        \end{equation*}

        This finishes the proof.
    \end{proof}


\section{Proof of converge rate of FW via JAGUAR Algorithm \ref{alg:FW}.} \label{appendix:FW}

    \begin{proof}[Proof of Theorem \ref{theorem:FW_nonstoch}]
        We start by writing our the result of Lemma 2 from \cite{mokhtari2020stochastic}. Under Assumptions \ref{ass:smooth}, \ref{ass:conv} the following inequality holds

        \begin{equation*}
            \expect{f(x^{k+1}) - f(x^*)} \leq (1 - \gamma_k) \expect{f(x^{k}) - f(x^*)} + \gamma_k D \expect{\norms{h^k - \nabla f(x^k)}} + \frac{L D^2 \gamma_k^2}{2}
        \end{equation*}

        We can evaluate $\expect{\norms{h^k - \nabla f(x^k)}}$ using Jensenâ€™s inequality:

        \begin{equation*}
            \expect{\norms{h^k - \nabla f(x^k)}} \leq \sqrt{\expect{\norms{h^k - \nabla f(x^k)}^2}}
        \end{equation*}

        Using result from Theorem \ref{theorem:JAGUAR_nonstoch} we can obtain

        \begin{equation*}
            \expect{\norms{h^k - \nabla f(x^k)}} 
            = 
            \mathcal{O} \left( \frac{d L D}{k + 8d} +
            \sqrt{d} L \gamma + \frac{\sqrt{d} \Delta}{\gamma} \right)
        \end{equation*}

        Using Lemma \ref{lem:recursion} with $\alpha_0 = 1, \beta_0 = 4, k_0 = 8d$;
        $\alpha_1 = 2, \beta_1 = 8 L D^2 + d L D^2$;
        $\alpha_2 = 1, \beta_2 = \sqrt{d} L \gamma D + \frac{\sqrt{d} \Delta D}{\gamma}$ and $i^* = 1$, we get:

        \begin{equation*}
            \expect{f(x^{k}) - f(x^*)} 
            =
            \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{k + 8d}
            + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right).
        \end{equation*}

        In Lemma \ref{lem:recursion} if $\alpha_0 = 1$ we need to take $\beta_0 \geq 2 \cdot 1 = 2$, we take $\beta_0 = 4$.


        This finishes the proof.
    \end{proof}

    \begin{proof}[Proof of Corollary \ref{cor:FW_nonstoch}]
        We aim to achieve precision $\varepsilon$, i.e.

        \begin{equation*}
            \expect{f(x^{N}) - f(x^*)} 
            =
            \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{N + 8d}
            + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right) \leq \varepsilon.
        \end{equation*}

        Therefore we need to take

        \begin{equation*}
            \begin{split}
                &N = \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{\varepsilon} \right),
                \\&\gamma = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right), \quad
                \Delta = \mathcal{O} \left( \frac{\varepsilon \gamma}{\sqrt{d} D}\right) = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right),
            \end{split}
            \end{equation*}
        
    \end{proof}
    

    \begin{proof}[Proof of Theorem \ref{theorem:FW}]
        Again we write out result of Lemma 2 from \cite{mokhtari2020stochastic}:

        \begin{equation}
        \label{eq:tmp_mokharti}
            \expect{f(x^{k+1}) - f(x^*)} \leq (1 - \gamma_k) \expect{f(x^{k}) - f(x^*)} + \gamma_k D \expect{\norms{g^k - \nabla f(x^k)}} + \frac{L D^2 \gamma_k^2}{2}
        \end{equation}

        We can evaluate $\expect{\norms{g^k - \nabla f(x^k)}}$ using Jensenâ€™s inequality:

        \begin{equation*}
            \expect{\norms{g^k - \nabla f(x^k)}} \leq \sqrt{\expect{\norms{g^k - \nabla f(x^k)}^2}}
        \end{equation*}

        Using result from Theorem \ref{theorem:JAGUAR} we can obtain

        \begin{equation*}
            \expect{\norms{g^k - \nabla f(x^k)}} 
            = 
            \mathcal{O} \left(\frac{L D + d \sigma_f/ \gamma + d\sigma_{\nabla}}{(k + 8d^{3/2})^{1/3}} 
            +
            \sqrt{d} L \gamma + \frac{\sqrt{d} \Delta}{\gamma} \right)
        \end{equation*}

        Set up $\gamma_k = \frac{4}{k + 8d^{3/2}}$ into \eqref{eq:tmp_mokharti}:

        \begin{equation*}
        \begin{split}
            \expect{f(x^{k+1}) - f(x^*)} 
            &\leq 
            (1 - \gamma_k) \expect{f(x^{k}) - f(x^*)} 
            + \frac{8 L D^2}{(k + 8d^{3/2})^2}
            \\&\quad+
            \frac{4D}{k + 8d^{3/2}}  \mathcal{O} \left(\frac{L D + d \sigma_f/ \gamma + d\sigma_{\nabla}}{(k + 8d^{3/2})^{1/3}} 
            + \sqrt{d} L \gamma + \frac{\sqrt{d} \Delta}{\gamma} \right)
        \end{split}
        \end{equation*}

        Using Lemma \ref{lem:recursion} with $\alpha_0 = 1, \beta_0 = 4, k_0 = 8d^{3/2}$;
        $\alpha_1 = 2, \beta_1 = 8 L D^2$;
        $\alpha_2 = 4/3; \beta_2 = L D + d \sigma_f/ \gamma + d\sigma_{\nabla}$;
        $\alpha_3 = 1, \beta_3 = \sqrt{d} L \gamma + \frac{\sqrt{d} \Delta}{\gamma}$ and $i^* = 2$, we get:

        \begin{equation*}
            \expect{f(x^{k}) - f(x^*)} 
            =
            \mathcal{O} \left( \frac{L D^2}{k + 8d^{3/2}} + \frac{\max\{L D^2 + d \sigma_f D/ \gamma + d\sigma_{\nabla} D, \sqrt{d} (f(x^0) - f(x^*))\}}{(k + 8d^{3/2})^{1/3}} 
            + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right).
        \end{equation*}

        In Lemma \ref{lem:recursion} if $\alpha_0 = 1$ we need to take $\beta_0 \geq 2 \cdot 1 = 2$, we take $\beta_0 = 4$.

        Since $k + 8d^{3/2} > (k + 8d^{3/2})^{1/3}$, we can obtain:

        \begin{equation*}
            \expect{f(x^{k}) - f(x^*)} 
            =
            \mathcal{O} \left( \frac{L D^2 + d \sigma_f D/ \gamma + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{(k + 8d^{3/2})^{1/3}} 
            + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right)
        \end{equation*}
        
        This finishes the proof.
    
    \end{proof}

    \begin{proof}[Proof of Corollary \ref{cor:FW}]
        We aim to achieve precision $\varepsilon$, i.e.

        \begin{equation*}
            \expect{f(x^{k}) - f(x^*)} 
            =
            \mathcal{O} \left( \frac{L D^2 + d \sigma_f D/ \gamma + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{(k + 8d^{3/2})^{1/3}} 
            + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right)
            \leq \varepsilon.
        \end{equation*}

        Therefore we need to take

        \begin{equation*}
                N = \mathcal{O} \left( \max\left\{ \left[ \frac{L D^2 + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{\varepsilon}\right]^3 , \frac{d^{9/2} \sigma_f^3 L^3D^6}{\varepsilon^6} \right\}\right),
            \end{equation*}
        \begin{equation*}
            \gamma = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right), \quad
            \Delta = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right).
        \end{equation*}
        
        
    \end{proof}