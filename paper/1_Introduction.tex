%%% Intriduction %%%

\section{Introduction}

    It will be rewritten because the topic has changed in the process.

    %Stochastic optimization is a fundamental problem that is encountered in a wide range of engineering applications, including machine learning, control theory, communication, etc. The goal is to minimize an objective function subject to general convex constraints.

    %In this paper, we consider a stochastic version of the method without projections, namely the momentum-based Frank-Wolf algorithm, with access to a zeroth-order oracle.

    %This paper also proposes an approach that allows you to use batches of arbitrary size, up to a single one. This will allow to use this method more effectively on weaker machines, allowing not to resort to distributed systems for computations. 

    %where $Q \subset \mathbb{R}^d$ is a convex and compact feasible set and $f(x, \xi)$ is stochastic function involving target variable $x \in \mathbb{R}^d$ and randomly distributed variable $\xi \sim D$. In this problem we consider that access to the gradient is very difficult or there is no access at all, so we can use only zeroth-order oracle of the function $f(\cdot, \xi)$. Specifically, we assume that on each step of our algorithms we can take only one or two points.