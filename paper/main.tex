\documentclass{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[main=english,russian]{babel}	
\usepackage{arxiv}

\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem-non}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma-non}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\input{macros}

\title{New Aspects of Black Box Conditional Gradient: Variance Reduction and One Point Feedback}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
    Alexander I. Bogdanov\\
	\texttt{bogdanov.ai@phystech.edu} \\
    \And
    Alexander N. Beznosikov\\
	\texttt{beznosikov.an@phystech.edu} \\
}

\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{New Aspects of Black Box Conditional Gradient}
\renewcommand{\arraystretch}{2.2}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf

\begin{document}
\maketitle

\begin{abstract}

    % In this paper we consider convex non-smooth stochastic minimization problem with constraints. We provide a gradient-free momentum-based Frank-Wolfe algorithm with one-point and two-point feedback. 

    In this paper, we address the challenges of solving a convex, non-smooth stochastic minimization problem subject to constraints, which often arises in modern machine learning applications. We propose a gradient-free momentum-based Frank-Wolfe algorithm that incorporates one-point and two-point feedback mechanisms, which have been shown to significantly improve convergence speed and robustness to noise, especially in the presence of large-scale and high-dimensional data. Specifically, our algorithm leverages the momentum term to accelerate convergence and overcome oscillations, while the feedback mechanism helps to adaptively adjust the step size, improving the overall performance. We provide theoretical analysis and numerical experiments to demonstrate the effectiveness and efficiency of our algorithm on various optimization problems, comparing it with existing algorithms such as the classical Frank-Wolfe and stochastic gradient descent methods.

\end{abstract}


% keywords can be removed
\keywords{Gradient-free methods \and Zeroth-order methods \and Stochastic optimization \and Frank-Wolfe algorithms \and Momentum-based method}

\input{1_introduction}
\input{2_Related_work}
\input{3_Main_results}
\input{4_Experiments}

        
\bibliographystyle{plain}
\bibliography{refs}  

\newpage

%\begin{appendices}

%    \part*{Appendix}
    
%    \input{5_Appendix}

%\end{appendices}


\begin{center}
    \LARGE \textbf{Appendix}
\end{center}
\normalsize

\appendix

\input{5_Appendix}
    
\end{document}