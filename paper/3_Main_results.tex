%%% Main results %%%

\section{Main results}

    \subsection{Jaguar gradient approximation. Non-stochastic case}
    \label{sect:JAGUAR_nonstoch}

        In this section we consider non-stochastic optimization problem 
    
        \begin{equation}
        \label{eq:problem_nonstoch}
            \underset{x \in Q}{\min} \quad f(x).
        \end{equation}

        where $Q \subseteq \mathbb{R}^d$ is arbitrary set.

        We assume that we have access only to zero-order oracle, i.e. we can only get values of functions $f(x)$, not of the gradient $\nabla f(x)$. This means that we need to approximate gradient of the function $f(x)$. 
        
        In practice, however, we usually do not even have access to $f(x)$, but only to its noisy version, i.e., we assume that zero-order oracle the returns to us the noisy values of the function $f(x)$ based on the point $x$ given to it, i.e. zero-order oracle returns $f_{\delta}(x) := f(x) + \delta(x)$.

        We define such difference scheme that will be used in our algorithm of gradient approximation

        \begin{equation}\label{eq:opf_nonstoch}
            \widetilde{\nabla}_if_\delta(x) :=  \dfrac{f_\delta(x + \gamma e_i) - f_\delta(x - \gamma e_i)}{2 \gamma} e_i,
        \end{equation}

        where $e_i$ is $i$-th basis vector from the standard basis in space $\mathbb{R}^d$, $\gamma$ is a smoothing parameter.

        How we can present algorithm of gradient approximation in the point $x^k$, where $x^k$ is a point on a step $k$ of any algorithm, that solves problem \eqref{eq:problem_nonstoch}:
    
        \begin{algorithm}[H]
    	\caption{JAGUAR gradient approximation. Non-stochastic case}
    	\label{alg:JAGUAR_nonstoch}
        	\begin{algorithmic}[1]
        		\State {\bf Input:} $x, h \in \mathbb{R}^d$
          
                \State Sample $i \in \overline{1, d}$ independently and uniform

                \State Compute $\widetilde{\nabla}_i f_{\delta}(x) = \frac{f_{\delta}(x + \gamma e_i) - f_{\delta}(x - \gamma e_i)}{2 \gamma} e_i$

                \State $h = h - \dotprod{h}{e_i} e_i + \widetilde{\nabla}_i f_{\delta}(x)$ \label{line:h^k_nonstoch}
    
                \State \textbf{Output:} $h$ 
        	\end{algorithmic}
        \end{algorithm}

        This approximation algorithm can be used for various iterative methods that, at each step $k$, obtain a new point $x^k$ that converges to the solution $x^*$ of the problem \ref{eq:problem_nonstoch}. Then we will also obtain the sequence $h^k$ of Line \ref{line:h^k_nonstoch}, which serves in a sense as a memory of the gradients from the previous iterations.

        We provide several assumptions required for the analysis:
    
        \begin{assumption}[Smoothness]\label{ass:smooth_nonstoch}
            The function $f(x)$ is $L$-smooth on a set $Q$, i.e. 
            
            \begin{equation*}
                \forall x, y \in Q \hookrightarrow \left\|\nabla f(x) - \nabla f(y)\right\| \leq L \left\|x-y\right\|,
            \end{equation*}
        \end{assumption}

        where $\norms{\cdot}$ is the standard Euclidean norm. We will use this notation throughout the paper.
    
        Because zero-order oracle returns to us noisy values of the function $f(x)$ we make common assumption on this noise.
    
        \begin{assumption}[Bounded oracle noise]\label{ass:bounded_nonstoch}
            \begin{equation*}
                \exists \Delta > 0 : ~\forall x \in Q \hookrightarrow |\delta(x)|^2 \leq \Delta^2
            \end{equation*}
        \end{assumption}

        Now we start to analyze convergence of JAGUAR gradient approximation \ref{alg:JAGUAR_nonstoch}. Our goal is to estimate the closeness of the true gradient $\nabla f(x^k)$ and the output of the JAGUAR algorithm $h^k$ at step $k$. However, first we need to introduce some auxiliary lemmas.

        \begin{lemma}
            \label{lemma:tilde_vs_notilda_nonstoch}
            Let us introduce the auxiliary notation
    
            \begin{equation}
            \label{eq:opf_d_nonstoch}
                \widetilde{\nabla}f_{\delta}(x) := \sum\limits_{i=1}^d \dfrac{f_{\delta}(x + \gamma e_i) - f_{\delta}(x - \gamma e_i)}{2 \gamma} e_i.
            \end{equation}
            
            Under Assumptions \ref{ass:smooth_nonstoch} and \ref{ass:bounded_nonstoch} the following inequality holds
    
            \begin{equation}
            \label{eq:diff_full_nonstoch}
                \norms{\widetilde{\nabla}f_{\delta}(x) - \nabla f(x)}^2
                \leq d L^2 \gamma^2 
                + \frac{2 d \Delta^2}{\gamma^2}.
            \end{equation}
        \end{lemma}
    
        For a detailed proof of Lemma \ref{lemma:tilde_vs_notilda_nonstoch}, see proof of Lemma \ref{lemma:tilde_vs_notilda} in Appendix \ref{appendix:JAGUAR} in case $\sigma_\nabla^2 = \sigma_f^2 = 0$ (see details in the Section \ref{sect:JAGUAR_stoch}). 
        The $\widetilde{\nabla}f_{\delta}(x)$ is a more precise version of approximation \eqref{eq:opf_nonstoch} as it approximates the gradient in every coordinate while \eqref{eq:opf_nonstoch} approximates only one, however approximation \eqref{eq:opf_d_nonstoch} requires $d$ times more zero-order oracle calls than \eqref{eq:opf_nonstoch}. However, we will show that utilising memory from previous iterations in the form of introducing a variable $h^k$ into our Algorithm \ref{alg:JAGUAR_nonstoch} in line \ref{line:h^k_nonstoch} will achieve the same accuracy as for $\widetilde{\nabla}f_{\delta}(x)$, but will only require two calls to the zero-order oracle at each iteration.
    
        %Now with the help of Lemma \ref{lemma:tilde_vs_notilda_nonstoch} we can write out one of the main lemma of this paper, it helps to evaluate the similarity between the truth gradient $\nabla f(x^k)$ and $h^k$, what we use instead of it.
    
        \begin{lemma}
        \label{lemma:h_vs_nablaf_nonstoch}
            Under Assumptions \ref{ass:smooth_nonstoch} and \ref{ass:bounded_nonstoch} the following inequality holds
            
            \begin{equation}
            \label{eq:h_vs_nabla_nonstoch}
            \begin{split}
            \expect{\norms{h^{k+1} - \nabla f(x^{k+1})}^2}
                    &\leq
                    \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
                    + 2d L^2 \expect{\norms{x^{k+1} - x^{k}}^2}
                    \\&\quad+ L^2 \gamma^2 
                    + \frac{2 \Delta^2}{\gamma^2}
            \end{split}
            \end{equation}
        
        \end{lemma}
    
        For a detailed proof of Lemma \ref{lemma:h_vs_nablaf_nonstoch}, see proof of Lemma \ref{lemma:h_vs_nablaf} in Appendix \ref{appendix:JAGUAR} in case $\sigma_\nabla^2 = \sigma_f^2 = 0$ (see details in the Section \ref{sect:JAGUAR_stoch}).
        Let us analyse formula \eqref{eq:h_vs_nabla_nonstoch}, and show that this result in the same estimate as in \eqref{eq:diff_full_nonstoch}. In many algorithms of optimization we can consider that 
        
        \begin{equation}
        \label{eq:ass_D}
            \norms{x^{k+1} - x^k}^2 \leq \gamma_k^2 D^2,
        \end{equation}
    
        where $\gamma_k$ is an optimizer step and $D^2$ is a constant, which depends on the optimisation algorithm. In the next section, we consider the Frank-Wolfe algorithm, where the diameter of the set $Q$ stands for $D$. 
        %If equation \eqref{eq:ass_D_nonstoch} is true, then we can take $\gamma_k \sim \frac{1}{k}$ and according to Lemma \ref{lem:recursion} we can observe:

        \begin{theorem}[Step tuning for JAGUAR. Non-stochastic case]
        \label{theorem:JAGUAR_nonstoch}
            Consider Assumptions \ref{ass:smooth} and \ref{ass:bounded}. If equation \eqref{eq:ass_D} is true, then we can take
            
            $$\gamma_k = \frac{4}{k + 8d},$$
            
            then we following convergence rate hold:

            \begin{equation*}
                \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = 
                \mathcal{O} \left( d L^2 \gamma^2 
                + \frac{d \Delta^2}{\gamma^2}
                +\frac{\max\{d^2 L^2 D^2, \norms{h^0 - \nabla f(x^0)}^2 \cdot d^2\}}{(k + d)^2} \right).
            \end{equation*}

            If $h_0 = \widetilde{\nabla} f_\delta(x^0) = \sum_{i=1}^d \dfrac{f_{\delta}(x + \gamma e_i) - f_{\delta}(x - \gamma e_i)}{2 \gamma} e_i$ we can obtain 
    
            \begin{equation*}
                \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = 
                \mathcal{O} \left( d L^2 \gamma^2 
                + \frac{d \Delta^2}{\gamma^2}
                +\frac{d^2 L^2 D^2}{(k + 8d)^2} \right).
            \end{equation*}
        
        \end{theorem}
        
        For detailed proof of Theorem \ref{theorem:FW_nonstoch} see Appendix \ref{appendix:JAGUAR_nonstoch}. From Theorem \ref{theorem:JAGUAR_nonstoch} we can conclude that after $\mathcal{O}\left(\frac{\sqrt{d} D}{\gamma} - d\right)$ steps we get exactly the same estimate as in equation \eqref{eq:diff_full_nonstoch}, but at each step except $k=0$ in algorithm JAGUAR \ref{alg:JAGUAR_nonstoch} we make two calls to the zero-order oracle, and to get estimate \eqref{eq:diff_full_nonstoch} we needed to make $2d$ oracle calls each step.

        In the next section, we consider the more general stochastic problem \eqref{eq:problem_stoch}. In this problem, we can no longer use $h^k$ as an approximation of the gradient, since it is in some sense biased.

    \subsection{Jaguar gradient approximation. Stochastic case}
    \label{sect:JAGUAR_stoch}

        In this section we consider stochastic optimization problem 
    
        \begin{equation}
        \label{eq:problem_stoch}
            \underset{x \in Q}{\min} \quad f(x) := 
            \mathbb{E}_{\xi}\left[f(x, \xi)\right],
        \end{equation}
    
        where $Q \subseteq \mathbb{R}^d$ is arbitrary set.
    
        %We assume that we have access only to zero-order oracle, i.e. we can only get values of functions $f(x, \xi)$, not of the gradient $\nabla f(x, \xi)$. This means that we need to approximate gradient of the function $f(x)$. 
        
        %In practice, however, we usually do not even have access to $f(x, \xi)$, but only to its noisy version, i.e., we assume that zero-order oracle the returns to us the noisy values of the function $f(x, \xi)$ based on the point $(x, \xi)$ given to it, i.e. zero-order oracle returns $f_{\delta}(x, \xi) := f(x, \xi) + \delta(x, \xi)$.

        In this Section, we also assume that we do not have access to the true value of the gradient $\nabla f(x, \xi)$, we only have access to the zero-order oracle, which returns the noisy value of the function $f(x, \xi)$: $f_{\delta}(x, \xi) := f(x, \xi) + \delta(x, \xi)$.
    
        In two point feedback (tpf) we define such gradient approximations of function $f(x)$:
    
        \begin{equation}\label{eq:tpf}
            \widetilde{\nabla}_if_\delta(x, \xi) :=  \dfrac{f_\delta(x + \gamma e_i, \xi) - f_\delta(x - \gamma e_i, \xi)}{2 \gamma} e_i,
        \end{equation}
    
        where $e_i$ is $i$-th basis vector from the standard basis in space $\mathbb{R}^d$, $\gamma$ is a smoothing parameter. In one point feedback (opf) we define slightly different gradient approximations function $f(x)$:
    
        \begin{equation}\label{eq:opf}
            \widetilde{\nabla}_if_\delta(x, \xi^+, \xi^-) :=  \dfrac{f_\delta(x + \gamma e_i, \xi^+) - f_\delta(x - \gamma e_i, \xi^-)}{2 \gamma} e_i
        \end{equation}
    
        The key difference between approximations \eqref{eq:tpf} and \eqref{eq:opf} is that Scheme \eqref{eq:tpf} is more accurate, but it is difficult to implement in practice because we have to get the same realization of $\xi$ at two different points $x + \gamma e$ and $x - \gamma e$, then Scheme \eqref{eq:opf} is more interesting from a practical point of view.
    
        To simplify further, we will assume that in the case of tpf we will have the same inscription as in opf, but only $\xi^+ = \xi^- = \xi$.
    
        How we can present algorithm of gradient approximation in the point $x^k$, where $x^k$ is a point on a step $k$ of any algorithm, that solves problem \eqref{eq:problem_stoch}:
    
        \begin{algorithm}[H]
    	\caption{JAGUAR gradient approximation. Stochastic case}
    	\label{alg:JAGUAR}
        	\begin{algorithmic}[1]
        		\State {\bf Input:} $x, h, g \in \mathbb{R}^d$, $0 \leq \eta \leq 1$

                \State Sample $i \in \overline{1, d}$ independently and uniform
          
                \State Sample 2 realizations of $\xi$: $\xi^+$ and $\xi^-$ independently (in tpf $\xi^+= \xi^-$)

                \State Compute $\widetilde{\nabla}_i f_{\delta}(x, \xi^+, \xi^-) = \frac{f_{\delta}(x + \gamma e_i, \xi^+) - f_{\delta}(x - \gamma e_i, \xi^-)}{2 \gamma} e_i$

                \State $h = h - \dotprod{h}{e_i} e_i + \widetilde{\nabla}_i f_{\delta}(x, \xi^+, \xi^-)$ \label{line:h^k}

                \State $\rho = h - d \cdot \dotprod{h}{e_i} e_i + d \cdot \widetilde{\nabla}_i f_{\delta}(x, \xi^+, \xi^-)$ \label{line:rho^k}

                \State $g = (1 - \eta) g + \eta \rho$ \label{line:g^k}
    
                \State \textbf{Output:} $h$ and $g$ 
        	\end{algorithmic}
        \end{algorithm}
    
        This Algorithm is similar to \ref{alg:JAGUAR_nonstoch}, but in Lines \ref{line:rho^k} and \ref{line:g^k} we need to use SEGA and  momentum parts in order to converge to the stochastic case. When applying this gradient approximation Algorithm \ref{alg:JAGUAR} to various iterative methods, we will now have not one additional $h^k$ sequence, but three: $h^k, g^k$, and $\eta_k$.
    
        We provide several assumptions required for the analysis:
    
        \begin{assumption}[Smoothness]\label{ass:smooth}
            The functions $f(x, \xi)$ are $L(\xi)$-smooth on a set $Q$, i.e. 
            \begin{equation*}
                \forall x, y \in Q \hookrightarrow \left\|\nabla f(x, \xi) - \nabla f(y, \xi)\right\| \leq L(\xi) \left\|x-y\right\|.
            \end{equation*}
    
            And exists constant $L^2$ such that 
            \begin{equation*}
                L^2 := \expect{L(\xi)^2}.
            \end{equation*}
        \end{assumption}
    
        If Assumption \ref{ass:smooth} holds, then function $f(x)$ is $L$-smooth on a set $Q$, since for all $x, y \in Q$ holds that
    
        \begin{equation*}
            \norms{\nabla f(x) - \nabla f(y)}^2 = \norms{\expect{\nabla f(x, \xi) - \nabla f(y, \xi)}}^2 \leq \expect{\norms{\nabla f(x, \xi) - \nabla f(y, \xi)}^2} \leq L^2 \norms{x - y}^2.
        \end{equation*}
    
        Because zero-order oracle returns to us noisy values of the function $f(x, \xi)$ we make common assumption on this noise.
    
        \begin{assumption}[Bounded oracle noise]\label{ass:bounded}
            \begin{equation*}
                \exists \Delta > 0 : ~\forall x \in Q \hookrightarrow \expect{|\delta(x, \xi)|^2} \leq \Delta^2
            \end{equation*}
        \end{assumption}
    
        If Assumption \ref{ass:bounded} holds, then if we define $\delta(x) := \expect{\delta(x, \xi)}$, then it holds that $|\delta(x)|^2 \leq \Delta^2$, since
    
        \begin{equation*}
            |\delta(x)|^2 = \left| \expect{\delta(x, \xi)} \right|^2 \leq \expect{|\delta(x, \xi)|^2} \leq \expect{\Delta^2} = \Delta^2.
        \end{equation*}

        Assumptions \ref{ass:smooth} and \ref{ass:bounded} are similar to Assumptions \ref{ass:smooth_nonstoch} and \ref{ass:bounded_nonstoch}, but we need to add random variable $\xi$ since we consider stochastic problem \eqref{eq:problem_stoch}. Now we present two assumptions that are needed only in stochastic case.
    
        \begin{assumption}[Bounded second moment of gradient] \label{ass:sigma_nabla}
            \begin{equation*}
                \exists \sigma^2_{\nabla} : \expect{\norms{\nabla f(x, \xi) - \nabla f(x)}^2} \leq \sigma^2_{\nabla}
            \end{equation*}
        \end{assumption}
    
        \begin{assumption}[Bounded second moment of function] \label{ass:sigma_f}
            \begin{equation*}
                \exists \sigma^2_{f} : \expect{\left|f(x, \xi) - f(x) \right|^2} \leq \sigma^2_{f}
            \end{equation*}
        \end{assumption}
    
        If two point feedback \eqref{eq:tpf} we will not need Assumption \ref{ass:sigma_f}, so for simplicity of future exposition we will assume that in the case of tpf Assumption \ref{ass:sigma_f} is fulfilled with $\sigma_f^2 = 0$.
    
        Now we start to analyze convergence of JAGUAR gradient approximation \ref{alg:JAGUAR} in stochastic case. First two lemmas of our analysis will be similar to Lemmas \ref{lemma:tilde_vs_notilda_nonstoch} and \ref{lemma:h_vs_nablaf_nonstoch}.
    
        \begin{lemma}
            \label{lemma:tilde_vs_notilda}
            Let us introduce the auxiliary notation
    
            \begin{equation}
            \label{eq:opf_d}
                \widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-) := \sum\limits_{i=1}^d \dfrac{f_{\delta}(x + \gamma e_i, \xi_i^+) - f_{\delta}(x - \gamma e_i, \xi_i^-)}{2 \gamma} e_i.
            \end{equation}
    
            In two point feedback \eqref{eq:tpf} $\xi_j^+ = \xi_j^-$.
            
            Under Assumptions \ref{ass:smooth}, \ref{ass:bounded}, \ref{ass:sigma_nabla} and \ref{ass:sigma_f} in opf case \eqref{eq:opf} the following inequality holds
    
            \begin{equation}
            \label{eq:diff_full}
                \expect{\norms{\widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-) - \nabla f(x)}^2} 
                \leq d L^2 \gamma^2 
                + \frac{8 d \sigma_f^2}{\gamma^2} 
                + 2 d \sigma_{\nabla}^2 + \frac{2 d \Delta^2}{\gamma^2}.
            \end{equation}
    
            In two point feedback \eqref{eq:tpf} $\sigma_f^2 = 0$.
        \end{lemma}
    
        For a detailed proof of Lemma \ref{lemma:tilde_vs_notilda}, see Appendix \ref{appendix:JAGUAR}. 
        %The $\widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-)$ is a more precise version of approximation \eqref{eq:opf} as it approximates the gradient in every coordinate while \eqref{eq:opf} approximates only one, however approximation \eqref{eq:opf_d} requires $d$ times more zero-order oracle calls than \eqref{eq:opf}. However, we will show that utilising memory from previous iterations in the form of introducing a variable $h^k$ into our Algorithm in line \ref{line:h^k} will achieve the same accuracy as for $\widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-)$, but will only require one call to the zero-order oracle at each iteration.
    
        %Now with the help of Lemma \ref{lemma:tilde_vs_notilda} we can write out one of the main lemma of this paper, it helps to evaluate the similarity between the truth gradient $\nabla f(x^k)$ and $h^k$, what we use instead of it.
    
        \begin{lemma}
        \label{lemma:h_vs_nablaf}
        Under Assumptions \ref{ass:smooth}, \ref{ass:bounded}, \ref{ass:sigma_nabla} and \ref{ass:sigma_f} in opf case \eqref{eq:opf} the following inequality holds
        
        \begin{equation}
        \label{eq:h_vs_nabla}
        \begin{split}
        \expect{\norms{h^{k+1} - \nabla f(x^{k+1})}^2}
                &\leq
                \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
                + 2d L^2 \expect{\norms{x^{k+1} - x^{k}}^2}
                \\&\quad+ L^2 \gamma^2 
                + \frac{8 \sigma_f^2}{\gamma^2} 
                + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2}
        \end{split}
        \end{equation}
    
        In two point feedback \eqref{eq:tpf} $\sigma^2_f = 0$
        \end{lemma}
    
        For a detailed proof of Lemma \ref{lemma:h_vs_nablaf}, see Appendix \ref{appendix:JAGUAR}. 
        %Let us analyse formula \eqref{eq:h_vs_nabla}, and show that this result in the same estimate as in \eqref{eq:diff_full}. In many algorithms of optimization we can consider that
    
        This Lemmas \ref{lemma:tilde_vs_notilda} and \ref{lemma:h_vs_nablaf} are similar to Lemmas \ref{lemma:tilde_vs_notilda_nonstoch} and \ref{lemma:h_vs_nablaf_nonstoch}, but summands with $\sigma^2_\nabla$ and $\sigma^2_f$ (in opf), which are related to stochasticity, interfere with the convergence of our Algorithm \ref{alg:JAGUAR}. It is for this reason that we need to introduce SAGA and moment correction in the form of a variables $\rho^k$ and $g^k$ in the Algorithm \ref{alg:JAGUAR} step on the lines \ref{line:rho^k} and \ref{line:g^k}. 
    
        \begin{lemma}
        \label{lemma:rho_vs_nablaf}
            Under Assumptions \ref{ass:smooth}, \ref{ass:bounded}, \ref{ass:sigma_nabla} and \ref{ass:sigma_f} in opf case \eqref{eq:opf} the following inequality holds
            
            \begin{equation*}
            \begin{split}
                \expect{\norms{\rho^k - \nabla f(x^k)}^2}
                &\leq
                4d \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}} 
                \\&+ 4d^2 \left( L^2 \gamma^2 
                + \frac{8 \sigma_f^2}{\gamma^2} 
                + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\gamma^2} \right)
                + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2}
            \end{split}
            \end{equation*}
        
            In two point feedback \eqref{eq:tpf} $\sigma^2_f = 0$
        \end{lemma}
    
        For a detailed proof of Lemma \ref{lemma:rho_vs_nablaf}, see Appendix \ref{appendix:JAGUAR}. As we can see, using SEGA line \ref{line:rho^k} deteriorates our estimates by a factor of $d$ compared to using $h^k$ as a gradient approximator, but in the stochastic case we care about the unbiased property, i.e. 
        
        $$\mathbb{E}_{i, \xi^+, \xi^-}[\rho^k] = \widetilde{\nabla} f_\delta(x^k) := \sum\limits_{i = 1}^{d} \frac{f_\delta (x + \gamma e_i) - f_\delta(x - \gamma e_i)}{2 \gamma} e_i$$
        
        Therefore we have to accept this factor.
        
        %The following theorem is the main theorem of this paper. It evaluates the closeness of the approximated $g^k$ gradient, which we obtained using the JAGUAR algorithm and the true gradient $\nabla f(x^k)$.
    
        \begin{lemma}
        \label{lemma:g_vs_nabla_f}
            Under Assumption \ref{ass:smooth} the following inequality holds
    
            \begin{equation*}
            \begin{split}
                \expect{\norms{g^k - \nabla f(x^k)}^2}
                &\leq 
                \left(1 - \eta_k\right) \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}
                +
                \frac{4 L^2}{\eta_k} \expect{\norms{x^k - x^{k-1}}^2}
                \\&\quad+
                \eta_k^2 \expect{\norms{\nabla f(x^k) - \rho^k}^2}
                +
                3 \eta_k \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2}
            \end{split}
            \end{equation*}
        \end{lemma}
    
        For a detailed proof of Lemma \ref{lemma:g_vs_nabla_f}, see Appendix \ref{appendix:JAGUAR}. 
    
        \begin{theorem}[Step tuning for JAGUAR. Stochastic case]
        \label{theorem:JAGUAR}
            Consider Assumptions \ref{ass:smooth}, \ref{ass:bounded}, \ref{ass:sigma_nabla} and \ref{ass:sigma_f} in opf case \eqref{eq:opf}.
            
            $$\gamma_k = \frac{4}{k + 8d^{3/2}} ~~\text{ and }~~ \eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}},$$
            
            If equation \eqref{eq:ass_D} is true, then the following inequality holds:
    
            %\begin{equation}
            %\label{eq:h_conv_final}
            %    \expect{\norms{h^{k} - \nabla f(x^{k})}^2} = \mathcal{O} \left( \frac{d^2 L^2 D^2}{(k + 8d^{3/2})^2}
            %    + d L^2 \gamma^2 + \frac{d \sigma_f^2}{\gamma^2}  + d \sigma_{\nabla}^2 + \frac{d \Delta^2}{\gamma^2} \right)
            %\end{equation}
    
            %\begin{equation}
            %\label{eq:rho_conv_final}
            %    \expect{\norms{\rho^{k} - \nabla f(x^{k})}^2} = \mathcal{O} \left(\frac{d^3 L^2 D^2}{(k + 8d^{3/2})^2}
            %    + d^2 L^2 \gamma^2 + \frac{d^2 \sigma_f^2}{\gamma^2}  + d^2 \sigma_{\nabla}^2 + \frac{d^2 \Delta^2}{\gamma^2} \right)
            %\end{equation}

            \begin{equation*}
            \begin{split}
                \expect{\norms{g^k - \nabla f(x^k)}^2} 
                &=
                \mathcal{O} \Bigg(\frac{L^2 D^2 + \max\{d^2 \sigma_f^2/ \gamma^2 + d^2 \sigma_{\nabla}^2, d \norms{g^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{2/3}}
                \\&\qquad\quad+
                \frac{d^4 \norms{h^0 - \nabla f(x^0)}^2}{(k + 8d^{3/2})^{8/3}}
                +
                d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2} \Bigg)
            \end{split}
            \end{equation*}

            If $h^0 = g^0 = \widetilde{\nabla} f_\delta(x^0, \xi^+_1, \xi^-_1, ..., \xi^+_d, \xi^-_d) = \sum_{i=1}^d \dfrac{f_{\delta}(x + \gamma e_i, \xi_i^+) - f_{\delta}(x - \gamma e_i, \xi_i^-)}{2 \gamma} e_i$ we can obtain
    
            \begin{equation*}
                \expect{\norms{g^k - \nabla f(x^k)}^2} 
                =
                \mathcal{O} \left(\frac{L^2 D^2 + d^2 \sigma_f^2/ \gamma^2 + d^2 \sigma_{\nabla}^2}{(k + 8d^{3/2})^{2/3}} 
                +
                d L^2 \gamma^2 + \frac{d \Delta^2}{\gamma^2} \right)
            \end{equation*}
    
            In two point feedback \eqref{eq:tpf} $\sigma^2_f = 0$.
            
        \end{theorem}
    
        For a detailed proof of Theorem \ref{theorem:JAGUAR}, see Appendix \ref{appendix:JAGUAR}. 

    \subsection{Frank-Wolfe via JAGUAR}
    \label{sect:FW_via_JAGUAR}

        In this section we consider the minimisation problem on the set $Q$. We make common assumptions required for analysis
    
        \begin{assumption}[Convex]\label{ass:conv}
            The objective function $f(x)$ is convex on a set $Q$, i.e. 
            \begin{equation*}
                \forall x, y \in Q \hookrightarrow f(y) \geq f(x) + \left<\nabla f(x), y - x\right>
            \end{equation*}
        \end{assumption}
    
        \begin{assumption}[Compact domain]\label{ass:compact}
            The set $Q$ is compact, i.e. 
    
            \begin{equation*}
                \exists D > 0 :~ \forall x, y \in Q \hookrightarrow \|x - y\| \leq D
            \end{equation*}
        \end{assumption}
    
        Now we can introduce Frank-Wolfe algorithm using JAGUAR approximation of the gradient
    
        \begin{algorithm}[H]
    	\caption{FW via JAGUAR}
    	\label{alg:FW}
        	\begin{algorithmic}[1]
        		\State {\bf Input, non-stochastic case:} $x_0 \in Q$, $g^0 = \widetilde\nabla f(x^0)$

                \State {\bf Input, stochastic case:} $x_0 \in Q$, $h^0 = g^0 = \widetilde\nabla f(x^0, \xi_1^+, ...., \xi_d^-)$, $\{\eta_k\}_{k=0}^N \subset [0; 1]$
        	    \For {k = 0, 1, 2, ... , N}
                    \State $g^{k+1} = $ JAGUAR $\left( x^k, h^k \right)$ \label{line:jaguar_nonstoch}
                    \Comment{Non-stochastic case}
                    \State $h^{k+1}, g^{k+1} = $ JAGUAR $\left( x^k, h^k, g^k, \eta_k \right)$ \label{line:jaguar_stoch}
                    \Comment{Stochastic case}
                    \State $s^k = \underset{x \in Q}{\arg\min}\left\{\left<s, g^{k+1} \right> \right\}$ \label{line:s^k}
                    \State $x^{k+1} = x^k + \gamma_k (s^k - x^k)$ \label{line:x^k}
                \EndFor
            \State \textbf{Output:} $x^{N+1}$ 
        	\end{algorithmic}
        \end{algorithm}
    
        %In line \ref{line:h^k+1} we use two gradient approximation $\widetilde{\nabla}_i f_{\delta}(x^k)$. In two point feedback we use \eqref{eq:tpf} and in one point feedback we use \eqref{eq:opf}.

        In Lines \ref{line:jaguar_nonstoch} and \ref{line:jaguar_stoch}  we use JAGUAR Algorithm \ref{alg:JAGUAR_nonstoch} if we consider non-stochastic problem \eqref{eq:problem_nonstoch} and Algorithm \ref{alg:JAGUAR} if we consider stochastic problem \eqref{eq:problem_stoch}. We have denoted the variable returned by the non-stochastic JAGUAR approximation \ref{alg:JAGUAR_nonstoch} in this algorithm as $g^k$, although in Section \ref{sect:JAGUAR_nonstoch} we denoted it as $h^k$, this was done for the general form of the Line \ref{line:s^k}. 
    
        We now explore the convergence of Algorithm \ref{alg:FW} in two problems \eqref{eq:problem_nonstoch} and \eqref{eq:problem_stoch}.

        \begin{theorem}[Convergence rate of FW via JAGUAR \ref{alg:FW}. Non-Stochastic case]
        \label{theorem:FW_nonstoch}
            Consider Assumptions \ref{ass:smooth_nonstoch}, \ref{ass:bounded_nonstoch}, \ref{ass:conv} and \ref{ass:compact}.
            If we take 
            
            $$\gamma_k = \frac{4}{k + 8d},$$
            
            then we FW via JAGUAR Algorithm \ref{alg:FW} in non-stochastic case \eqref{eq:problem_nonstoch} has the following convergence rate
    
            \begin{equation*}
                \expect{f(x^{N}) - f(x^*)} 
                =
                \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{N + 8d}
                + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right).
            \end{equation*}
        \end{theorem}

        For a detailed proof of Theorem \ref{theorem:FW_nonstoch}, see Appendix \ref{appendix:FW}.

        \begin{corollary}
        \label{cor:FW_nonstoch}
            Let Assumptions from Theorem \ref{theorem:FW_nonstoch} be satisfied, then Algorithm \ref{alg:FW} in non-stochastic case \eqref{eq:problem_nonstoch} has the following convergence rate

            \begin{equation*}
                N = \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{\varepsilon} \right), \quad
                \gamma = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right), \quad
                \Delta = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right),
            \end{equation*}

            where $\varepsilon$ is desired accuracy, i.e. $\expect{f(x^N) - f(x^*)} \leq \varepsilon$.

        \end{corollary}

        For a detailed proof of Corollary \ref{cor:FW_nonstoch}, see Appendix \ref{appendix:FW}.
    
        \begin{theorem}[Convergence rate of FW via JAGUAR \ref{alg:FW}. Stochastic case]
        \label{theorem:FW}
            Consider Assumptions \ref{ass:smooth}, \ref{ass:bounded}, \ref{ass:sigma_nabla}, \ref{ass:conv}, \ref{ass:compact} and \ref{ass:sigma_f} in opf case \eqref{eq:opf}.
            If we take 
            
            $$\gamma_k = \frac{4}{k + 8d^{3/2}} ~~\text{ and }~~ \eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}},$$
            
            then we FW via JAGUAR Algorithm \ref{alg:FW} has the following convergence rate
    
            \begin{equation*}
                \expect{f(x^{N}) - f(x^*)} 
                =
                \mathcal{O} \left( \frac{L D^2 + d \sigma_f D/ \gamma + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{(N + 8d^{3/2})^{1/3}} 
                + \sqrt{d} L D \gamma + \frac{\sqrt{d} \Delta D}{\gamma}\right)
            \end{equation*}
    
            In two point feedback \eqref{eq:tpf} $\sigma^2_f = 0$.
            
        \end{theorem}
    
        For a detailed proof of Theorem \ref{theorem:FW}, see Appendix \ref{appendix:FW}.

        \begin{corollary}
        \label{cor:FW}
            Let Assumptions from Theorem \ref{theorem:FW} be satisfied, then Algorithm \ref{alg:FW} in stochastic case \eqref{eq:problem_stoch} has the following convergence rate

            \begin{equation*}
                N = \mathcal{O} \left( \max\left\{ \left[ \frac{L D^2 + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{\varepsilon}\right]^3 , \frac{d^{9/2} \sigma_f^3 L^3D^6}{\varepsilon^6} \right\}\right),
            \end{equation*}
            \begin{equation*}
                \gamma = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right), \quad
                \Delta = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right),
            \end{equation*}

            where $\varepsilon$ is desired accuracy, i.e. $\expect{f(x^N) - f(x^*)} \leq \varepsilon$. In two point feedback \eqref{eq:tpf} $\sigma_f^2 = 0$ and convergence on $N$ takes form

            \begin{equation*}
                N = \mathcal{O} \left( \left[ \frac{L D^2 + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{\varepsilon}\right]^3 \right).
            \end{equation*}
        \end{corollary}

        For a detailed proof of Corollary \ref{cor:FW}, see Appendix \ref{appendix:FW}.