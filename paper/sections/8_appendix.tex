\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\section{Вспомогательные леммы и факты}

\subsection{Квадрат нормы суммы} \label{axil:squared}
    
    Для всех $x_1, \dots , x_n \in \mathbb{R}^n$, где $n \in \{2, 4\}$:
    \begin{align*}
        \left\| x_1 + x_2 + \dots + x_n \right\|^2 \leq n \left\| x_1 \right\|^2 + \dots + n \left\| x_n \right\|^2. 
    \end{align*}

\subsection{Неравенство Коши-Шварца} \label{axil:cauchy_schwarz}

    Для всех $x, y \in \mathbb{R}^d$:
    \begin{align*}
        \langle x, y \rangle \leq \left\| x \right\| \left\| y \right\|.
    \end{align*}

    \subsection{Неравенства Юнга-Фенхеля} \label{axil:fenchel_young}
    
    Для всех $x, y \in \mathbb{R}^d$ и $\beta > 0$:
    \begin{align*}
        2 \langle x, y \rangle \leq \beta^{-1} \| x \|^2 + \beta \| y \|^2.
    \end{align*}

\subsection{Лемма о рекурсии}
    
    \begin{lemma} \label{lemma:baskirskaya_lemma}
    
        Для всех $x \in [0; 1)$ рассматривается функцию
        \begin{align*}
            \phi(x) := 1 - (1 - x)^{\alpha} - \max \{1, \alpha\} x.
        \end{align*}
        
        Тогда для всех $0 \leq x < 1$ и $\alpha \in \mathbb{R}$ можно получить, что $\phi(x) \leq 0$.
        
    \end{lemma}
    
    \begin{proof}
%         First consider the case of $\alpha \notin (0; 1)$. Then we can write out Bernoulli's inequality: for all  $x < 1$ it holds that

%         \begin{equation*}
%             (1 - x)^{\alpha} \geq 1 - \alpha x.
%         \end{equation*}

%         Therefore for $0 \leq x < 1$:

%         \begin{equation*}
%             \phi(x) = 1 - (1 - x)^{\alpha} - \max\{1, \alpha\} x \leq 1 - (1 - x)^{\alpha} - \alpha x \leq 0.
%         \end{equation*}

%         Now we consider case $0 < \alpha < 1$, therefore $\phi(x)$ takes the form

%         \begin{equation*}
%             \phi(x) = 1 - (1 - x)^{\alpha} -  x.
%         \end{equation*}

%         Note that

%         \begin{equation*}
%             \phi''(x) = \alpha(1 - \alpha)(1 - x)^{\alpha-2} > 0.
%         \end{equation*}

%         Therefore $\phi(x)$ is convex on a segment $[0; 1]$ and $\psi(0) = \psi(1) = 0$, that means that $\phi(x) \leq 0$ for all $x \in [0; 1)$. This finishes the proof.
    \end{proof}

    \begin{lemma}[Лемма о рекурсии] \label{lemma:recursion}
%     Suppose we have the following recurrence relation for variables $\{r_k\}_{k=0}^N \subset \mathbb{R}$

%     \begin{equation}
%     \label{eq:recur_start}
%         r_{k+1} \leq \left(1 - \frac{\beta_0}{(k + k_0)^{\alpha_0}}\right) r_{k} + \sum\limits_{i = 1}^m \frac{\beta_i}{(k + k_0)^{\alpha_i}},
%     \end{equation}

%     where $\beta_i > 0 ~~ \forall i \in \overline{0, m}$, $0 \leq \alpha_0 \leq 1$, $\alpha_i \in \mathbb{R} ~~ \forall i \in \overline{1, m}$. 
    
%     Then we can estimate the convergence of the sequence $\{r_k\}_{k=0}^N$ to zero:

%     \begin{equation}
%     \label{eq:recur_end}
%         r_k \leq 2 \cdot \sum\limits_{i = 1}^m \frac{Q_i}{(k+k_0)^{\alpha_i - \alpha_0}},
%     \end{equation}

%     where $Q_{i^*} = \max\{\beta_{i^*} / \beta_0, r_0 k_0^{\alpha_{i^*} - \alpha_0}\}$ and $Q_i = \beta_i / \beta_0$ if $i \neq i^*$, where $i^*$ we can choose arbitrarily from the set $\overline{1, m}$, and

%     \begin{itemize}
%         \item[$\bullet$] if $~0 \leq \alpha_0 < 1$:

%         \begin{equation*}
%             k_0 \geq \left( \frac{2}{\beta_0} \max\{1, \max\{\alpha_i\} - \alpha_0\} \right)^{\frac{1}{1 - \alpha_0}} ~~ \text{ and } ~~ \beta_0 > 0.
%         \end{equation*}

%         \item[$\bullet$] if $~\alpha_0 = 1$:

%         \begin{equation*}
%             k_0 \in \mathbb{N} ~~\text{ and }~~ \beta_0 \geq 2 \max\{1, \max\{\alpha_i\} - 1\}.
%         \end{equation*}
%     \end{itemize}
        
    \end{lemma}
    \begin{proof}
%         We prove the claim in \eqref{eq:recur_end} by induction. First, note that

%         \begin{equation*}
%             r_0 = r_0 \cdot \left(\frac{k_0}{0 + k_0}\right)^{\alpha_{i^*} - \alpha_0} 
%             \leq 
%             \frac{Q_{i^*}}{(0 + k_0)^{\alpha_{i^*} - \alpha_0}}
%             \leq
%             2 \cdot \sum\limits_{i = 0}^m \frac{Q_i}{(0 + k_0)^{\alpha_i - \alpha_0}}.
%         \end{equation*}
        
%         and therefore the base step of the induction holds true. 
        
%         Now assume that the condition in \eqref{eq:recur_end} holds for some $k$. Now we will show that this condition will hold for $k + 1$.

%         We start by fitting \eqref{eq:recur_end} into the original recurrence relation \eqref{eq:recur_start} and using that $\beta_i \leq Q_i \beta_0$:

%         \begin{equation*}
%         \begin{split}
%             r_{k+1} &\leq
%             \left(1 - \frac{\beta_0}{(k + k_0)^{\alpha_0}}\right) \cdot \left(2 \sum\limits_{i = 1}^m \frac{Q_i}{(k + k_0)^{\alpha_i - \alpha_0}} \right) + \sum\limits_{i=1}^m \frac{\beta_i}{(k + k_0)^{\alpha_i}}
%             \\&\leq
%             2 \sum\limits_{i = 1}^m \frac{Q_i}{(k + k_0)^{\alpha_i - \alpha_0}}
%             -
%             \sum\limits_{i = 1}^m \frac{Q_i \beta_0}{(k + k_0)^{\alpha_i}}
%             =
%             \sum\limits_{i = 1}^m \left(\frac{2 Q_i}{(k + k_0)^{\alpha_i - \alpha_0}} - \frac{Q_i \beta_0}{(k + k_0)^{\alpha_i}} \right).
%         \end{split}
%         \end{equation*}

%         Our goal is to show that for all $i \in \overline{1, m}$ it holds that 

%         \begin{equation}
%         \label{eq:desired_ineq}
%             \frac{2 Q_i}{(k + k_0)^{\alpha_i - \alpha_0}} - \frac{Q_i \beta_0}{(k + k_0)^{\alpha_i}} \leq \frac{2 Q_i}{(k + k_0 + 1)^{\alpha_i - \alpha_0}}.
%         \end{equation}

%         Let us rewrite this inequality in such a way that it takes a more convenient form:

%         \begin{equation*}
%             \frac{2}{\beta_0}\underbrace{ \left[  1 - \left( 1 - \frac{1}{k + k_0 + 1}\right)^{\alpha_i - \alpha_0} \right]}_{\circledOne} \leq \left( \frac{1}{k+k_0}\right)^{\alpha_0}.
%         \end{equation*}

%         Using Lemma \ref{lem:baskirskaya_lemma} with $x = (k + k_0 + 1)^{-1} \in [0; 1)$ and $\alpha = \alpha_i - \alpha_0$ we can obtain that

%         \begin{equation*}
%             \circledOne \leq \max\{1, \alpha_i - \alpha_0\} \frac{1}{k+k_0 + 1} \leq \max\{1, \alpha_i - \alpha_0\} \frac{1}{k + k_0}.
%         \end{equation*}

%         Now our desired inequality \eqref{eq:desired_ineq} takes form 

%         \begin{equation*}
%             \frac{2}{\beta_0} \max\{1, \alpha_i - \alpha_0\} \frac{1}{k + k_0} \leq \left( \frac{1}{k+k_0}\right)^{\alpha_0}.
%         \end{equation*}

%         Again, we rewrite it in a more convenient form:

%         \begin{equation}
%         \label{eq:desired_ineq_final}
%             \frac{2}{\beta_0} \max\{1, \alpha_i - \alpha_0\} \leq (k + k_0)^{1 - \alpha_0}.
%         \end{equation}

%         Now consider two cases

%         \begin{itemize}
%             \item[$\bullet$] If $~0 \leq \alpha_0 < 1$.

%             In this case $(k + k_0)^{1 - \alpha_0} \geq k_0^{1 - \alpha_0}$ and if we take

%             \begin{equation*}
%                 k_0 \geq \left( \frac{2}{\beta_0} \max\{1, \max\{\alpha_i\} - \alpha_0\} \right)^{\frac{1}{1 - \alpha_0}},
%             \end{equation*}

%             then according to \eqref{eq:desired_ineq_final} desired inequality \eqref{eq:desired_ineq} will be fulfilled for all $i \in \overline{1, m}$ for all $\beta_0 > 0$. 

%             \item[$\bullet$] If $~\alpha_0 = 1$, then inequality \eqref{eq:desired_ineq_final} takes form

%             \begin{equation*}
%                 \frac{2}{\beta_0} \max\{1, \alpha_i - 1\} \leq 1.
%             \end{equation*}

%             Therefore if we take

%             \begin{equation*}
%                 \beta_0 \geq 2 \max\{1, \max\{\alpha_i\} - 1\},
%             \end{equation*}

%             then again according to \eqref{eq:desired_ineq_final} desired inequality \eqref{eq:desired_ineq} will be fulfilled for all $i \in \overline{1, m}$ for all $k_0 \in \mathbb{N}$.
%         \end{itemize}

%         This finishes the proof.
    \end{proof}

\section{Доказательство сходимости \texttt{JAGUAR}} \label{appendix:JAGUAR}

\subsection{Доказательство Леммы \ref{lemma:tilde_vs_notilda}}

%         Let's start by writing out a definition of gradient approximation $\widetilde{\nabla}f_{\delta}(x, \xi_{\overline{1, d}}^\pm)$:
        
%         \begin{equation*}
%         \begin{split}
%             &\expect{\norms{\widetilde{\nabla}f_{\delta}(x, \xi_{\overline{1, d}}^\pm) - \nabla f(x)}^2}
%             = 
%             \expect{\norms{\sum\limits_{i = 1}^d \frac{f_{\delta}(x + \tau e_i, \xi^+_i) - f_{\delta}(x - \tau e_i, \xi^-_i)}{2 \tau}e_i - \nabla f(x)}^2}
%             \\&= 
%             \expect{\norms{\sum\limits_{i = 1}^d \left(\frac{f_{\delta}(x + \tau e_i, \xi^+_i) - f_{\delta}(x - \tau e_i, \xi^-_i)}{2 \tau} - \dotprod{\nabla f(x)}{e_i} \right) e_i}^2}
%             \\&\overset{(\star)}{=}
%             \sum\limits_{i = 1}^d \expect{\norms{\left(\frac{f_{\delta}(x + \tau e_i, \xi^+_i) - f_{\delta}(x - \tau e_i, \xi^-_i)}{2 \tau} - \dotprod{\nabla f(x)}{e_i} \right) e_i}^2}
%             \\&=
%             \sum\limits_{i = 1}^d \expect{\left|\frac{f_{\delta}(x + \tau e_i, \xi^+_i) - f_{\delta}(x - \tau e_i, \xi^-_i)}{2 \tau} - \dotprod{\nabla f(x)}{e_i} \right|^2} .
%     \end{split}
%     \end{equation*}

%     The $(\star)$ equality holds since $\dotprod{e_i}{e_j} = 0$ if $i \neq j$. Now let's estimate the value under the summation:

%     \begin{equation*}
%     \begin{split}
%         &\expect{\left|\frac{f_{\delta}(x + \tau e_i, \xi^+_i) - f_{\delta}(x - \tau e_i, \xi^-_i)}{2 \tau} - \dotprod{\nabla f(x)}{e_i} \right|^2} 
%         \\&= 
%         \mathbb{E}\Bigg[ \Bigg|\frac{f(x + \tau e_i, \xi^+_i) - f(x - \tau e_i, \xi^-_i)}{2 \tau} - \dotprod{\nabla f(x)}{e_i}
%         + 
%         \frac{\delta(x + \tau e_i, \xi^+_i) - \delta(x - \tau e_i, \xi^-_i)}{2 \tau}\Bigg|^2 \Bigg]
%         \\&\overset{\ref{axil:squared}}{\leq}
%         \frac{1}{2 \tau^2} \underbrace{\expect{\left|f(x + \tau e_i, \xi^+_i) - f(x - \tau e_i, \xi^-_i) - \dotprod{\nabla f(x)}{2 \tau e_i} \right|^2}}_{\circledOne}
%         + 
%         \frac{2 \Delta^2}{\tau^2} .
%     \end{split}
%     \end{equation*}

%         Last inequality holds since noise is bounded . Consider $\circledOne$. Using \ref{axil:squared} with $n = 4$ we get:

%         \begin{equation}
%         \label{eq:tmp_lemma_zlp}
%         \begin{split}
%             &\expect{\left| f(x + \tau e_i, \xi^+_i) - f(x - \tau e_i, \xi^-_i) - \dotprod{\nabla f(x)}{2 \tau e_i} \right|^2}
%             \\&\leq
%             4 \expect{\left| f(x + \tau e_i, \xi^+_i) - f(x, \xi^+_i) - \dotprod{\nabla f(x, \xi^+_i)}{ \tau e_i} \right|^2}
%             \\&+
%             4 \expect{\left|- f(x - \tau e_i, \xi^-_i) + f(x, \xi^-_i) + \dotprod{\nabla f(x, \xi^-_i)}{-\tau e_i} \right|^2}
%             \\&+
%             4 \expect{\left| f(x, \xi^+_i) - f(x, \xi^-_i) \right|^2}
%             \\&+ 
%             4 \expect{\left| \dotprod{\nabla f(x, \xi^+_i) + \nabla f(x, \xi^-_i) - 2 \nabla f(x)}{\tau e_i} \right|^2} .
%         \end{split}
%         \end{equation}

%         Let's evaluate all these four components separately. Since functions $f(x, \xi^+_i)$ and $f(x, \xi^-_i)$ are $L(\xi^{\pm}_i)$-smooth we have estimates for first and second:
%         \begin{equation}
%         \label{eq:tmp_first_and_second_zlp}
%         \begin{split}
%             &\left| f(x + \tau e_i, \xi^+_i) - f(x, \xi^+_i) - \dotprod{\nabla f(x, \xi^+_i)}{ \tau e_i} \right| \leq \frac{L(\xi_i^+)}{2} \tau^2 \leq \frac{L}{2} \tau^2,
%             \\&\left|- f(x - \tau e_i, \xi^-_i) + f(x, \xi^-_i) + \dotprod{\nabla f(x, \xi^-_i)}{-\tau e_i} \right| \leq \frac{L(\xi_i^-)}{2} \tau^2 \leq \frac{L}{2} \tau^2 .
%         \end{split}
%         \end{equation}

%         If we consider TPF approximation \eqref{eq:tpf}, then third term in \eqref{eq:tmp_lemma_zlp} equals to zero, since $\xi^+_i = \xi^-_i$, if we consider the OPF case \eqref{eq:opf}, then we can obtain

%         \begin{equation}
%         \label{eq:tmp_third_zlp}
%             \expect{\left| f(x, \xi^+_i) - f(x, \xi^-_i) \right|^2}
%             \leq 2\expect{\left| f(x, \xi^+_i) - f(x) \right|^2} + 2\expect{\left| f(x, \xi^-_i) - f(x) \right|^2} \leq 4 \sigma_f^2 .
%         \end{equation}

%         Consider the last point in \eqref{eq:tmp_lemma_zlp} and using Cauchy–Schwarz inequality \ref{axil:cauchy_schwarz} we can obtain:

%         \begin{equation}
%         \label{eq:tmp_four_zlp}
%             \expect{\left| \dotprod{\nabla f(x, \xi^+_i) - \nabla f(x)}{\tau e_i} \right|^2}
%             \leq
%             \expect{\norms{\nabla f(x, \xi^+_i) - \nabla f(x)}^2 \tau^2}
%             \leq \sigma_{\nabla}^2 \tau^2 .
%         \end{equation}

%         Combining \eqref{eq:tmp_first_and_second_zlp}, \eqref{eq:tmp_third_zlp} and \eqref{eq:tmp_four_zlp} we obtain

%         \begin{equation*}
%             \expect{\norms{\widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-) - \nabla f(x)}^2} 
%             \leq d L^2 \tau^2 
%             + \frac{8 d \sigma_f^2}{\tau^2} 
%             + 2 d \sigma_{\nabla}^2 + \frac{2 d \Delta^2}{\tau^2} .
%         \end{equation*}

%         In the case of two point feedback $\sigma_f^2 = 0$ and in the deterministic case \eqref{eq:problem_nonstoch} $\sigma_\nabla^2 = \sigma_f^2 = 0$. This finishes the proof.

\subsection{Доказательство Леммы \ref{lemma:h_vs_nablaf}}

%         Let us start by writing out a definition of $h^{k}$ using line \ref{line:h^k} of Algorithm \ref{alg:FW_stoch}

%         \begin{equation*}
%         \begin{split}
%             &\expect{\norms{h^{k} - \nabla f(x^{k})}^2} 
%             =
%             \expect{\norms{h^{k-1} + \widetilde{\nabla}_i f_{\delta}(x^k, \xi^+, \xi^-) - \dotprod{h^{k-1}}{e_i} e_i - \nabla f(x^{k})}^2}
%             \\&=
%             \mathbb{E}\Bigg[\Bigg\|
%             \left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right) 
%             \\&\qquad\quad+ e_i e_i^T \left(\widetilde{\nabla} f_{\delta}(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k) \right)
%             \\&\qquad\quad- \left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)\Bigg\|^2\Bigg]
%             \\&=
%             \underbrace{\expect{\norms{\left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right) }^2}}_{\circledOne}
%             \\&\quad+ \underbrace{\expect{\norms{e_i e_i^T \left(\widetilde{\nabla} f_{\delta}(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k) \right)}^2}}_{\circledTwo}
%             \\&\quad+
%             \underbrace{\expect{\norms{\left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)}^2}}_{\circledThree}
%             \\&\quad+
%             \underbrace{\expect{2\dotprod{\left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right)}{\left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)}^2}}_{\circledFour} .
%         \end{split}
%         \end{equation*}

%         In the last equality the two remaining scalar products are zero, since  $e_i e_i^T \left(I - e_i e_i^T\right) = e_i^T e_i - e_i^T e_i = 0$. Consider the $\circledOne$. Using notation $v := h^{k-1} - \nabla f(x^{k-1})$ we obtain

%         \begin{equation*}
%         \begin{split}
%             \expect{\norms{\left(I - e_i e_i^T\right) \left(h^{k-1} - \nabla f(x^{k-1}) \right)}^2}
%             &=
%             \expect{v^T\left(I - e_i e_i^T\right)^T \left(I - e_i e_i^T\right) v}
%             \\&=
%             \expect{v^T\left(I - e_i e_i^T\right) v}
%             = \expect{\mathbb{E}_{k-1}\left[ v^T\left(I - e_i e_i^T\right) v \right]},
%         \end{split}
%         \end{equation*}

%         where $\mathbb{E}_{k-1}[\cdot]$ is the conditional expectation with fixed randomness of all steps up to $k-1$. Since at step $k$ the vectors $e_i$ are generated independently, we obtain

%         \begin{equation*}
%         \begin{split}
%             \expect{\mathbb{E}_{k-1}\left[ v^T\left(I - e_i e_i^T\right) v \right]}
%             &=
%             \expect{v^T\mathbb{E}_{k-1}\left[\left(I - e_i e_i^T\right) \right] v} 
%             \\&= 
%             \left(1 - \frac{1}{d}\right) \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}^2} .
%         \end{split}
%         \end{equation*}

%         Consider $\circledTwo$. Since we generate $i$ independently, $x^k$ is independent of the $e_i$ generated at step $k$, then we can apply the same technique as in estimation $\circledOne$:

%         \begin{equation*}
%         \begin{split}
%             &\expect{\norms{e_i e_i^T \left(\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k) \right)}^2} 
%             \\&=
%             \frac{1}{d} \expect{\norms{\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k)}^2} .
%         \end{split}
%         \end{equation*}

%         Using Lemma \ref{lemma:tilde_vs_notilda} we obtain 

%         \begin{equation*}
%             \frac{1}{d} \expect{\norms{\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ..., \xi^+, \xi^-) - \nabla f(x^k)}^2}
%             \leq
%             L^2 \tau^2 
%             + \frac{8 \sigma_f^2}{\tau^2} 
%             + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2} .
%         \end{equation*}

%         Consider $\circledThree$. Using the same technique as in estimation $\circledOne$:

%         \begin{equation*}
%             \expect{\norms{\left(I - e_i e_i^T\right) \left(\nabla f(x^{k}) - \nabla f(x^{k-1})\right)}^2} \leq \left(1 - \frac{1}{d}\right) L^2 \expect{\norms{x^k - x^{k-1}}^2} .
%         \end{equation*}

%         Consider $\circledFour$. Using Fenchel-Young inequality \ref{axil:fenchel_young} with $\beta = 2d$ we obtain

%         \begin{equation*}
%             \circledFour \leq \left(1 - \frac{1}{d}\right) \left(\frac{1}{2d} \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}^2} + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2}\right) .
%         \end{equation*}

%         Therefore it holds that

%         \begin{equation*}
%         \begin{split}
%             \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
%             &\leq
%             \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k-1} - \nabla f(x^{k-1})}^2}
%             + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2}
%             \\&\quad+ L^2 \tau^2 
%             + \frac{8 \sigma_f^2}{\tau^2} 
%             + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2} .
%         \end{split}
%         \end{equation*}

%         This finishes the proof.

\subsection{Доказательство Леммы \ref{lemma:rho_vs_nablaf}}
    
%         Let's start by writing out a definition of $\rho^{k}$ using line \ref{line:rho^k} of Algorithm \ref{alg:FW_stoch}

%         \begin{equation*}
%         \begin{split}
%             &\expect{\norms{\rho^k - \nabla f(x^k)}^2}
%             =
%             \expect{\norms{h^{k-1} + d \widetilde{\nabla}_i f_\delta (x^k, \xi^+, \xi^-) - d \dotprod{h^{k-1}}{e_i} e_i - \nabla f(x^k)}^2}
%             \\&=
%             \mathbb{E}\Bigg[\Bigg\|(I - d e_i e_i^T)\left(h^{k-1} - \nabla f(x^{k-1})\right) 
%             + 
%             de_ie_i^T \left( \widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ... , \xi^+, \xi^-) - \nabla f(x^k)\right)
%             \\&\qquad\quad+ 
%             (I - d e_i e_i^T)\left(\nabla f(x^{k-1}) - \nabla f(x^k)\right)
%             \Bigg\|^2\Bigg]
%             \\&\overset{\star}{\leq}
%             4 (d-1) \expect{\norms{h^{k-1} - \nabla f(x^{k-1}}} 
%             + 4 d \expect{\norms{\widetilde{\nabla} f_\delta(x^k, \xi^+, \xi^-, ... , \xi^+, \xi^-) - \nabla f(x^k)}^2} 
%             \\&\quad+ 2 (d-1) \expect{\norms{\nabla f(x^{k-1}) - \nabla f(x^k)}^2} .
%         \end{split}
%         \end{equation*}

%         The $\star$ inequality is correct due to similar reasoning as in the proof of Lemma \ref{lemma:h_vs_nablaf} and due to Fenchel-Young inequality \ref{axil:fenchel_young}. Now we can estimate all three summands using Lemmas \ref{lemma:h_vs_nablaf} and \ref{lemma:tilde_vs_notilda} and using Assumption \ref{ass:smooth}:

%         \begin{equation*}
%         \begin{split}
%             \expect{\norms{\rho^k - \nabla f(x^k)}^2}
%             &\leq
%             4d \expect{\norms{h^{k-1} - \nabla f(x^{k-1}}} 
%             \\&+ 4d^2 \left( L^2 \tau^2 
%             + \frac{8 \sigma_f^2}{\tau^2} 
%             + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2} \right)
%             + 2d L^2 \expect{\norms{x^k - x^{k-1}}^2} .
%         \end{split}
%         \end{equation*}

%         This finishes the proof.

\subsection{Доказательство Леммы \ref{lemma:g_vs_nabla_f}}
    
%         We start by writing out a definition of $g^k$ using line \ref{line:g^k} of Algorithm \ref{alg:FW_stoch}

%         \begin{align*}
%             &\expect{\norms{g^k - \nabla f(x^k)}^2}
%             =
%             \expect{\norms{\nabla f(x^{k-1}) - g^{k-1} + \nabla f(x^k) - \nabla f(x^{k-1}) - \left( g^k - g^{k-1} \right)}^2}
%             \notag\\
%             &=
%             \expect{\norms{\nabla f(x^{k-1}) - g^{k-1} + \nabla f(x^k) - \nabla f(x^{k-1}) - \eta_k\left( \rho^k - g^{k-1} \right)}^2}
%             \notag\\
%             &=
%             \expect{\norms{(1-\eta_k)(\nabla f(x^{k-1}) - g^{k-1}) 
%             + (1 - \eta_k) (\nabla f(x^k) - \nabla f(x^{k-1})) + \eta_k \left( \nabla f(x^k) - \rho^k \right)}^2}
%             \notag\\
%             &=
%             (1-\eta_k)^2 \underbrace{\expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}}_{\circledOne} 
%             + 
%             (1-\eta_k)^2 \underbrace{\expect{\norms{\nabla f(x^k) - \nabla f(x^{k-1})}^2}}_{\circledTwo} 
%             \notag\\
%             &\quad+
%             \eta_k^2 \underbrace{\expect{\norms{\nabla f(x^k) - \rho^k}^2}}_{\circledThree} 
%             +
%             2(1 - \eta_k)^2\underbrace{\expect{\dotprod{\nabla f(x^{k-1}) - g^{k-1}}{\nabla f(x^k) - \nabla f(x^{k-1})}}}_{\circledFour}
%             \notag\\
%             &\quad+
%             2 \eta_k(1 - \eta_k)\underbrace{\expect{\dotprod{\nabla f(x^{k-1}) - g^{k-1}}{\nabla f(x^k) - \rho^k}}}_{\circledFive}
%             \notag\\
%             &\quad+
%             2 \eta_k(1 - \eta_k)\underbrace{\expect{\dotprod{\nabla f(x^k) - \nabla f(x^{k-1})}{\nabla f(x^k) - \rho^k}}}_{\circledSix} .
%         \end{align*}

%         Consider $\circledFive$. Since we generate $\xi^+$ and $\xi^-$ independently, we obtain 

%         \begin{equation*}
%             \circledFive = \expect{\dotprod{\nabla f(x^{k-1}) - g^{k-1}}{\mathbb{E}_{k - 1} \left[\nabla f(x^k) - \rho^k \right]}},
%         \end{equation*}

%         where $\mathbb{E}_{k-1}[\cdot]$ is the conditional expectation with fixed randomness of all steps up to $k-1$. Using fact that 

%         \begin{equation*}
%             \mathbb{E}_{k - 1} \left[\nabla f(x^k) - \rho^k \right] = \nabla f(x^k) - \widetilde{\nabla} f(x^k) = \nabla f(x^k) - \sum\limits_{i = 1}^d \frac{f(x + \tau e_i) - f(x - \tau e_i)}{2 \tau} e_i.
%         \end{equation*}

%         Fact that for $\widetilde{\nabla} f_\delta(x)$ Lemma \ref{lemma:tilde_vs_notilda} holds true with $\sigma_f^2 = \sigma_\nabla^2 = 0$ and using Cauchy Schwarz inequality \ref{axil:cauchy_schwarz} with $\beta = 2 (1 - \eta_k)$ we can assume

%         \begin{equation}
%         \label{eq:tmp_th3_5}
%             \circledFive \leq \frac{1}{4(1 - \eta_k)} \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2} + (1 - \eta_k) \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2}.
%         \end{equation}

%         Similarly it can be shown that 

%         \begin{equation}
%         \label{eq:tmp_th3_6}
%             \circledSix \leq \frac{1}{2 (1 - \eta_k) \eta_k^2} \expect{\norms{\nabla f(x^k) - \nabla f(x^{k-1})}^2} 
%             +
%             \frac{(1 - \eta_k) \eta_k^2}{2} \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2}.
%         \end{equation}

%         Using Assumption \ref{ass:smooth} we can obtain that 

%         \begin{equation}
%         \label{eq:tmp_th3_3}
%             \circledTwo \leq L^2 \expect{\norms{x^k - x^{k-1}}^2}.
%         \end{equation}

%         Consider $\circledFour$. Using auchy Schwarz inequality \ref{axil:cauchy_schwarz} with $\beta = 2 \frac{(1 - \eta_k)^2}{\eta_k}$ we can assume

%         \begin{equation}
%         \label{eq:tmp_th3_4}
%             \circledFour \leq \frac{\eta_k}{4 (1 - \eta_k)^2} \expect{\norms{g^k - \nabla f(x^k)}^2}
%             +
%             \frac{(1 - \eta_k)^2}{\eta_k} L^2 \expect{\norms{x^k - x^{k-1}}^2} .
%         \end{equation}

%         Putting \eqref{eq:tmp_th3_5}, \eqref{eq:tmp_th3_6}, \eqref{eq:tmp_th3_3} and \eqref{eq:tmp_th3_4} all together and using the fact that $(1 -\eta_k)^2 \leq 1 - \eta_k$, we obtain

%         \begin{equation*}
%         \begin{split}
%             \expect{\norms{g^k - \nabla f(x^k)}^2}
%             &\leq 
%             \left(1 - \eta_k\right) \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}
%             +
%             \frac{4 L^2}{\eta_k} \expect{\norms{x^k - x^{k-1}}^2}
%             \\&\quad+
%             \eta_k^2 \expect{\norms{\nabla f(x^k) - \rho^k}^2}
%             +
%             3 \eta_k \expect{\norms{\widetilde{\nabla} f_\delta(x^k) - \nabla f(x^k)}^2} .
%         \end{split}
%         \end{equation*}

%         This finishes the proof.

\section{Доказательство сходимости алгоритма Франка-Вульфа с \texttt{JAGUAR}} \label{appendix:FW}

\subsection{Доказательство Теоремы \ref{theorem:JAGUAR_nonstoch}}
        
    Начнем с того, что выпишем результат из Леммы \ref{lemma:h_vs_nablaf} с $\sigma_f = \sigma_\nabla = 0$ и подставим $\gamma_k = \frac{4}{k + k_0}$:
    \begin{align*}
        \mathbb{E} \left[ \left\| h^{k + 1} - \nabla f(x^{k + 1}) \right\|^2 \right] \leq \left(1 - \frac{1}{2 d} \right)
        & \mathbb{E}
        \left[ \left\| h^k - \nabla f(x^k) \right\|^2 \right] \\
        &+ 
        \frac{32 d L^2 D^2}{(k + k_0)^2} + L^2 \tau^2 + \frac{2 \Delta^2}{\tau^2}.
    \end{align*}
    
    Теперь используем Лемму \ref{lemma:recursion} с $\alpha_0 = 0, \beta_0 = 1/2d$; $\alpha_1 = 2, \beta_1 = 32d L^2 D^2$; $\alpha_2 = 0, \beta_2 = L^2 \tau^2 + \frac{2 \Delta^2}{\tau^2}$ и $i^* = 1$:
    \begin{align*}
        \mathbb{E} \left[ \left\| h^k - \nabla f(x^k) \right\|^2 \right] = \mathcal{O} \left( d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2} + \frac{\max \{d^2 L^2 D^2, \left\| h^0 - \nabla f(x^0) \right\|^2 \cdot k_0^2\}}{(k + k_0)^2} \right),
        \end{align*}
    где $k_0 = (4d \cdot 2)^1 = 8d$. Если $h_0 = \widetilde{\nabla} f_\delta (x^0)$, то получим:
    \begin{align*}
        \mathbb{E} \left[ \left\| h^k - \nabla f(x^k) \right\|^2 \right] = \mathcal{O} \left( d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2} + \frac{d^2 L^2 D^2}{(k + 8d)^2} \right).
    \end{align*}

    На этом доказательство закончено.
        
%     \begin{proof}[Proof of Theorem \ref{theorem:FW_nonstoch}]
%         We start by writing our the result of Lemma 2 from \cite{mokhtari2020stochastic}. Under Assumptions \ref{ass:smooth}, \ref{ass:conv} the following inequality holds

%         \begin{align*}
%             \expect{f(x^{k+1}) - f(x^*)} \leq (1 - \gamma_k) \expect{f(x^{k}) - f(x^*)} + \gamma_k D \expect{\norms{h^k - \nabla f(x^k)}} + \frac{L D^2 \gamma_k^2}{2} .
%         \end{align*}

%         We can evaluate $\expect{\norms{h^k - \nabla f(x^k)}}$ using Jensen’s inequality:

%         \begin{align*}
%             \expect{\norms{h^k - \nabla f(x^k)}} \leq \sqrt{\expect{\norms{h^k - \nabla f(x^k)}^2}} .
%         \end{align*}

%         Using result from Theorem \ref{theorem:JAGUAR_nonstoch} we can obtain

%         \begin{align*}
%             \expect{\norms{h^k - \nabla f(x^k)}} 
%             = 
%             \mathcal{O} \left( \frac{d L D}{k + 8d} +
%             \sqrt{d} L \tau + \frac{\sqrt{d} \Delta}{\tau} \right) .
%         \end{align*}

%         Using Lemma \ref{lem:recursion} with $\alpha_0 = 1, \beta_0 = 4, k_0 = 8d$;
%         $\alpha_1 = 2, \beta_1 = 8 L D^2 + d L D^2$;
%         $\alpha_2 = 1, \beta_2 = \sqrt{d} L \tau D + \frac{\sqrt{d} \Delta D}{\tau}$ and $i^* = 1$, we get:

%         \begin{align*}
%             \expect{f(x^{k}) - f(x^*)} 
%             =
%             \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{k + 8d}
%             + \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau}\right).
%         \end{align*}

%         In Lemma \ref{lem:recursion} if $\alpha_0 = 1$ we need to take $\beta_0 \geq 2 \cdot 1 = 2$, we take $\beta_0 = 4$. This finishes the proof.
%     \end{proof}

%     \begin{proof}[Proof of Corollary \ref{cor:FW_nonstoch}]
%         We aim to achieve precision $\varepsilon$, i.e.

%         \begin{align*}
%             \expect{f(x^{N}) - f(x^*)} 
%             =
%             \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{N + 8d}
%             + \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau}\right) \leq \varepsilon.
%         \end{align*}

%         Therefore we need to take

%         \begin{align*}
%             \begin{split}
%                 &N = \mathcal{O} \left( \frac{d \max\{L D^2, f(x^0) - f(x^*)\}}{\varepsilon} \right),
%                 \\&\tau = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right), \quad
%                 \Delta = \mathcal{O} \left( \frac{\varepsilon \tau}{\sqrt{d} D}\right) = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right) .
%             \end{split}
%             \end{align*}

%         This finishes the proof.
%     \end{proof}

%     \begin{proof}[Proof of Theorem \ref{theorem:JAGUAR}]
%         Consider $\expect{\norms{h^{k} - \nabla f(x^{k})}^2}$. We start by writing out result from Lemma \ref{lemma:h_vs_nablaf} and setting up $\gamma_k = \frac{4}{k + k_0}$:

%         \begin{align*}
%         \begin{split}
%             \expect{\norms{h^{k+1} - \nabla f(x^{k+1})}^2}
%             &\leq
%             \left(1 - \frac{1}{2 d}\right) \expect{\norms{h^{k} - \nabla f(x^{k})}^2}
%             + \frac{32 d L^2 D^2}{(k + k_0)^2} 
%             \\&\quad+ L^2 \tau^2 
%             + \frac{8 \sigma_f^2}{\tau^2} 
%             + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2} .
%         \end{split}
%         \end{align*}

%         Now we use Lemma \ref{lem:recursion} with $\alpha_0 = 0, \beta_0 = 1/2d$;
%         $\alpha_1 = 2, \beta_1 = 32d L^2 D^2$;
%         $\alpha_2 = 0, \beta_2 = L^2 \tau^2 + \frac{8 \sigma_f^2}{\tau^2} + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2}$ and $i^* = 1$.

%         \begin{align*}
%         \begin{split}
%             &\expect{\norms{h^{k} - \nabla f(x^{k})}^2} 
%             \\&= 
%             \mathcal{O} \left( d L^2 \tau^2 
%             + \frac{d \sigma_f^2}{\tau^2} 
%             + d \sigma_{\nabla}^2 + \frac{d \Delta^2}{\tau^2}
%             +\frac{\max\{d^2 L^2 D^2, \norms{h^0 - \nabla f(x^0)}^2 \cdot k_0^2\}}{(k + k_0)^2} \right),
%         \end{split}
%         \end{align*}

%         where $k_0 = (4d \cdot 2)^1 = 8d$. For simplicity of calculations further we take $k_0 = 8 d^{3/2} > 8d$. If $h^0 = \widetilde{\nabla} f_\delta(x^0, \xi^+_1, \xi^-_1, ..., \xi^+_d, \xi^-_d)$ we can obtain

%         \begin{align*}
%         \begin{split}
%             &\expect{\norms{h^{k} - \nabla f(x^{k})}^2} 
%             = 
%             \mathcal{O} \left( d L^2 \tau^2 
%             + \frac{d \sigma_f^2}{\tau^2} 
%             + d \sigma_{\nabla}^2 + \frac{d \Delta^2}{\tau^2}
%             +\frac{d^2 L^2 D^2}{(k + 8d^{3/2})^2} \right) .
%         \end{split}
%         \end{align*}

%         Consider $\expect{\norms{\rho^{k} - \nabla f(x^{k})}^2}$. Using Lemmas \ref{lemma:rho_vs_nablaf} and \ref{lemma:tilde_vs_notilda} we obtain

%         \begin{align*}
%         \begin{split}
%             &\expect{\norms{\rho^{k} - \nabla f(x^{k})}^2} 
%             \\&= 
%             \mathcal{O} \left( d^2 L^2 \tau^2 
%             + \frac{d^2 \sigma_f^2}{\tau^2} 
%             + d^2 \sigma_{\nabla}^2 + \frac{d^2 \Delta^2}{\tau^2}
%             +\frac{d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^2} \right) .
%         \end{split}
%         \end{align*}

%         Consider $\expect{\norms{g^k - \nabla f(x^k)}^2}$. We write out result from Lemma \ref{lemma:g_vs_nabla_f} and setting up $\eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}}$:

%         \begin{align*}
%         \begin{split}
%             &\expect{\norms{g^k - \nabla f(x^k)}^2}
%             \leq 
%             \left(1 - \eta_k\right) \expect{\norms{\nabla f(x^{k-1}) - g^{k-1}}^2}
%             +
%             \frac{4 L^2 D^2}{(k + 8d^{3/2})^{4/3}}
%             \\&+
%             \frac{4}{(k + 8d^{3/2})^{4/3}} \mathcal{O} \left(d^2 L^2 \tau^2 
%             + \frac{d^2 (\sigma_f^2 + \Delta^2)}{\tau^2} 
%             + d^2 \sigma_{\nabla}^2 
%             +\frac{d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^2} \right)
%             \\&+
%             \frac{12}{(k + 8d^{3/2})^{2/3}} \left( d L^2 \tau^2  
%             + \frac{d \Delta^2}{\tau^2}\right) .
%         \end{split}
%         \end{align*}

%         Using Lemma \ref{lem:recursion} with $\alpha_0 = 2/3, \beta_0 = 4$; 
%         $\alpha_1 = 4/3, \beta_1 = 4 L^2 D^2$; 
%         $\alpha_2 = 4/3, \beta_2 = 4 d^2 L^2 \tau^2 + \frac{4 d^2 \sigma_f^2}{\tau^2} + 4 d^2 \sigma_{\nabla}^2 + \frac{4 d^2 \Delta^2}{\tau^2}$;
%         $\alpha_3 = 10/3, \beta_3 = 4 d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}$;
%         $\alpha_4 = 2/3, \beta_4 = d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2}$ and $i^* = 2$ we get:

%         \begin{align}
%         \label{eq:tmp_last_1}
%         \begin{split}
%             &\expect{\norms{g^k - \nabla f(x^k)}^2} 
%             \\&= 
%             \mathcal{O} \Bigg(\frac{L^2 D^2 + \max\{d^2 L^2 \tau^2 + d^2 \sigma_f^2/ \tau^2 + d^2 \sigma_{\nabla}^2 + d^2 \Delta^2 / \tau^2, d \norms{g^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{2/3}} 
%             \\&\qquad \quad+
%             \frac{d^3 \max\{L^2 D^2, d \norms{h^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{8/3}} + d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2} \Bigg) .
%         \end{split}
%         \end{align}

%         Since 
        
%         $$\frac{d^3 L^2 D^2}{(k + 8d^{3/2})^{8/3}} \leq \frac{L^2 D^2}{(k + 8d^{3/2})^{2/3}} ~~\text{ and }~~
%         \frac{d^2 L^2 \tau^2 + d^2 \Delta^2 / \tau^2}{(k + 8d^{3/2})^{2/3}} \leq d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2},$$

%         we can simplify \eqref{eq:tmp_last_1}:

%         \begin{align*}
%         \begin{split}
%             \expect{\norms{g^k - \nabla f(x^k)}^2} 
%             &=
%             \mathcal{O} \Bigg(\frac{L^2 D^2 + \max\{d^2 \sigma_f^2/ \tau^2 + d^2 \sigma_{\nabla}^2, d \norms{g^0 - \nabla f(x^0)}^2\}}{(k + 8d^{3/2})^{2/3}}
%             \\&\qquad\quad+
%             \frac{d^4 \norms{h^0 - \nabla f(x^0)}^2}{(k + 8d^{3/2})^{8/3}}
%             +
%             d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2} \Bigg) .
%         \end{split}
%         \end{align*}

%         If $h^0 = g^0 = \widetilde{\nabla} f_\delta(x^0, \xi^+_1, \xi^-_1, ..., \xi^+_d, \xi^-_d)$ we can obtain

%         \begin{align*}
%             \expect{\norms{g^k - \nabla f(x^k)}^2} 
%             =
%             \mathcal{O} \left(\frac{L^2 D^2 + d^2 \sigma_f^2/ \tau^2 + d^2 \sigma_{\nabla}^2}{(k + 8d^{3/2})^{2/3}} 
%             +
%             d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2} \right) .
%         \end{align*}

%         This finishes the proof.
%     \end{proof}
    
%     \begin{proof}[Proof of Theorem \ref{theorem:FW}]
%         Again we write out result of Lemma 2 from \cite{mokhtari2020stochastic}:

%         \begin{align}
%         \label{eq:tmp_mokharti}
%             \expect{f(x^{k+1}) - f(x^*)} \leq (1 - \gamma_k) \expect{f(x^{k}) - f(x^*)} + \gamma_k D \expect{\norms{g^k - \nabla f(x^k)}} + \frac{L D^2 \gamma_k^2}{2} .
%         \end{align}

%         We can evaluate $\expect{\norms{g^k - \nabla f(x^k)}}$ using Jensen’s inequality:

%         \begin{align*}
%             \expect{\norms{g^k - \nabla f(x^k)}} \leq \sqrt{\expect{\norms{g^k - \nabla f(x^k)}^2}} .
%         \end{align*}

%         Using result from Theorem \ref{theorem:JAGUAR} we can obtain

%         \begin{align*}
%             \expect{\norms{g^k - \nabla f(x^k)}} 
%             = 
%             \mathcal{O} \left(\frac{L D + d \sigma_f/ \tau + d\sigma_{\nabla}}{(k + 8d^{3/2})^{1/3}} 
%             +
%             \sqrt{d} L \tau + \frac{\sqrt{d} \Delta}{\tau} \right) .
%         \end{align*}

%         Set up $\gamma_k = \frac{4}{k + 8d^{3/2}}$ into \eqref{eq:tmp_mokharti}:

%         \begin{align*}
%         \begin{split}
%             \expect{f(x^{k+1}) - f(x^*)} 
%             &\leq 
%             (1 - \gamma_k) \expect{f(x^{k}) - f(x^*)} 
%             + \frac{8 L D^2}{(k + 8d^{3/2})^2}
%             \\&\quad+
%             \frac{4D}{k + 8d^{3/2}}  \mathcal{O} \left(\frac{L D + d \sigma_f/ \tau + d\sigma_{\nabla}}{(k + 8d^{3/2})^{1/3}} 
%             + \sqrt{d} L \tau + \frac{\sqrt{d} \Delta}{\tau} \right) .
%         \end{split}
%         \end{align*}

%         Using Lemma \ref{lem:recursion} with $\alpha_0 = 1, \beta_0 = 4, k_0 = 8d^{3/2}$;
%         $\alpha_1 = 2, \beta_1 = 8 L D^2$;
%         $\alpha_2 = 4/3; \beta_2 = L D + d \sigma_f/ \tau + d\sigma_{\nabla}$;
%         $\alpha_3 = 1, \beta_3 = \sqrt{d} L \tau + \frac{\sqrt{d} \Delta}{\tau}$ and $i^* = 2$, we get:

%         \begin{align*}
%         \begin{split}
%             &\expect{f(x^{k}) - f(x^*)} 
%             \\&=
%             \mathcal{O} \Bigg( \frac{L D^2}{k + 8d^{3/2}} + \frac{\max\{L D^2 + d \sigma_f D/ \tau + d\sigma_{\nabla} D, \sqrt{d} (f(x^0) - f(x^*))\}}{(k + 8d^{3/2})^{1/3}} 
%             \\&\qquad\quad+ \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau}\Bigg).
%         \end{split}
%         \end{align*}

%         In Lemma \ref{lem:recursion} if $\alpha_0 = 1$ we need to take $\beta_0 \geq 2 \cdot 1 = 2$, we take $\beta_0 = 4$. Since $k + 8d^{3/2} > (k + 8d^{3/2})^{1/3}$, we can obtain:

%         \begin{align*}
%         \begin{split}
%             &\expect{f(x^{k}) - f(x^*)} 
%             \\&=
%             \mathcal{O} \left( \frac{L D^2 + d \sigma_f D/ \tau + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{(k + 8d^{3/2})^{1/3}} 
%             + \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau}\right) .
%         \end{split}
%         \end{align*}
        
%         This finishes the proof.
%     \end{proof}

%     \begin{proof}[Proof of Corollary \ref{cor:FW}]
%         We aim to achieve precision $\varepsilon$, i.e.

%         \begin{align*}
%         \begin{split}
%             &\expect{f(x^{k}) - f(x^*)} 
%             \\&=
%             \mathcal{O} \left( \frac{L D^2 + d \sigma_f D/ \tau + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{(k + 8d^{3/2})^{1/3}} 
%             + \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau}\right)
%             \leq \varepsilon.
%         \end{split}
%         \end{align*}

%         Therefore we need to take

%         \begin{align*}
%                 N = \mathcal{O} \left( \max\left\{ \left[ \frac{L D^2 + d\sigma_{\nabla} D + \sqrt{d} (f(x^0) - f(x^*))}{\varepsilon}\right]^3 , \frac{d^{9/2} \sigma_f^3 L^3D^6}{\varepsilon^6} \right\}\right),
%             \end{align*}
%         \begin{align*}
%             \tau = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right), \quad
%             \Delta = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right).
%         \end{align*}
        
%         This finishes the proof.
%     \end{proof}