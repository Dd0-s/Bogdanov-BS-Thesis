\section{Введение}

    Методы без проекций, такие как условный градиент, известный как алгоритм Франка-Вульфа (ФВ)\cite{frank1956algorithm}, широко используются для решения различных задач оптимизации. В последнее десятилетие методы условного градиента вызывают все больший интерес в сообществе машинного обучения, поскольку во многих случаях вычислительно дешевле решить линейную задачу минимизации на подходящем выпуклом множестве (например, на $l_p$-шарах или симплексе $\Delta_d$), а затем сделать проекцию на него. \cite{leblanc1985improved, jaggi2011sparse, bubeck2015convex, hazan2016introduction, goldfarb2017linear, dadras2022federated, freund2017extended}.
    
    В оригинальной работе Франка-Вульфа \cite{frank1956algorithm} авторы использовали истинный градиент в своем алгоритме, однако современные задачи машинного обучения и искусственного интеллекта требуют использования различных оценок градиента, что связано со значительным увеличением размера датасетов и сложности современных моделей. Примерами таких градиентных оценок в алгоритмах типа ФВ являются координатные методы \cite{lacoste2013block, wang2016parallel, osokin2016minding} и стохастическая градиентная аппроксимация с батчами \cite{reddi2016stochastic, zhang2020one, lu2021generalized}.

    Но иногда встречаются еще более сложные ситуации, когда мы не можем вычислить градиент в общем случае, потому что он недоступен по разным причинам, например, целевая функция не дифференцируема или вычисление градиента вычислительно сложно \cite{taskar2005learning, chen2017zoo, nesterov2017random, choromanski2018structured, fazel2018global}. Такая постановка называется оптимизацией "черного ящика"\ \cite{lian2015asynchronous}, и в этом случае мы вынуждены использовать методы оценки градиента нулевого порядка через конечные разности функции цели (иногда с дополнительным шумом) для аппроксимации градиента \cite{doi:10.1137/100802001, duchi2012randomized}. 

    За последние годы исследований по теме оптимизации "черного ящика"\ можно выделить два основных метода аппроксимации градиента с помощью конечных разностей. Первый оценивает градиент в $m$ координатах \cite{richtarik2014iteration, wright2015coordinate, doi:10.1137/16M1060182}:
    \begin{align} \label{eq:turtle_approx}
        \frac{d}{m} \sum_{i \in I} \frac{f(x + \tau e_i) - f(x - \tau e_i)}{2 \tau} e_i,
    \end{align} 
    где $I \subset \overline{1, d} : |I| = m$, $e_i$ -- вектор из стандартного базиса в $\mathbb{R}^d$ и $\tau$ -- параметр сглаживания.
    
    Эта конечная разность аппроксимирует градиент в координатах $m$ и требует $\mathcal{O}(m)$ вызовов оракула. Если $m$ мало, то такая оценка будет неточной, если $m$ велико, то на каждой итерации нужно делать много обращений к оракулу нулевого порядка. В случае $m = d$ мы называем этот метод \textit{полная аппроксимация}.

    Второй использует в конечной разности не стандартный базис, а случайные вектора $e$ \cite{duchi2012randomized, nesterov2017random, gasnikov2022power, Randomized_gradient_free_methods_in_convex_optimization}:
    \begin{align} \label{eq:l2_approx}
        d \frac{f(x + \tau e) - f(x - \tau e)}{2 \tau} e,
    \end{align}
    где $e$ может быть равномерно распределено на $l_p$-сфере $RS^d_p(1)$, тогда эта схема называется $l_p$-\textit{сглаживание}. В последних работах авторы обычно используют $p = 1$ \cite{gasnikov2016gradient-free, akhavan2022gradient} или $p=2$ \cite{nemirovskij1983problem, shamir2017optimal, doi:10.1137/19M1259225}. Кроме того, $e$ может быть взято из нормального распределения с нулевым средним и единичной ковариационной матрицей \cite{nesterov2017random}.

    Аппроксимации \eqref{eq:turtle_approx} и \eqref{eq:l2_approx} имеют очень большую дисперсию или требуют много обращений к нулевому оракулу, поэтому возникает необходимость как-то уменьшить ошибку аппроксимации, не увеличивая при этом количество обращений к нулевому оракулу. В стохастической оптимизации довольно широко используется метод запоминания информации с предыдущих итераций, например, в SVRG \cite{johnson2013accelerating}, SAGA \cite{defazio2014saga}, SARAH \cite{nguyen2017sarah} и SEGA \cite{hanzely2018sega} авторы предлагают запоминать градиент с предыдущих итераций для лучшей сходимости метода.
    Я решил использовать эту технику в задаче оптимизации "черного ящика"\ и запоминать градиентные аппроксимации из предыдущих итераций для уменьшения размера батча без существенной потери точности.

    В этой работе я попытаюсь ответить на следующие вопросы:

    \begin{itemize}
        \item \textit{Можно ли создать метод нулевого порядка, который будет использовать информацию из предыдущих итераций и аппроксимировать истинный градиент так же точно, как и полная аппроксимация \eqref{eq:turtle_approx}, но потребует $\mathcal{O}(1)$ вызовов оракула нулевого порядка?}
        \item \textit{Можно ли реализовать этот метод аппроксимации в алгоритме Франка-Вольфа для детерминированных и стохастических постановок задач минимизации?}
        \item \textit{Является ли оценка сходимости этого метода лучше, чем для разностных схем \eqref{eq:turtle_approx} и \eqref{eq:l2_approx}?}
    \end{itemize}

   В более реалистичной постановке оракул нулевого порядка возвращает зашумленное значение целевой функции, то есть выдает не $f(x)$, а $f(x) + \delta(x)$. В литературе рассматриваются различные виды шума $\delta(\cdot)$: он может быть стохастическим \cite{bach2016highly, doi:10.1137/19M1259225, akhavan2020exploiting, gasnikov2022power} или детерминированным \cite{risteski2016algorithms, bogolubsky2016learning, sahu2018distributed, bayandina2018gradient, dvinskikh2022noisy, lobanov2023non}. Поэтому возникает еще один исследовательский вопрос:

    \begin{itemize}
        \item \textit{Как различные типы шума влияют на теоретические гарантии и практические результаты для предложенных мной подходов?}
    \end{itemize}

\subsection{Мой вклад}
    
    В соответствии с вопросами исследования, мой вклад может быть обобщен следующим образом:

    \begin{itemize}
        \item Я представляю метод \texttt{JAGUAR}, который аппроксимирует истинный градиент целевой функции $\nabla f(x)$ в точке $x$. Использование памяти предыдущих итераций позволяет достичь точности, близкой к полной аппроксимации \eqref{eq:turtle_approx}, но \texttt{JAGUAR} требует не $\mathcal{O}(d)$, а $\mathcal{O}(1)$ обращений к оракулу нулевого порядка. Сглаживание $l_p$ \eqref{eq:l2_approx} также требует $\mathcal{O}(1)$ обращения к оракулу, но поскольку в нем нет техники памяти, этот метод имеет большую дисперсию и не является робастным.
        \item Я доказал теоретические оценки для этого метода (см. раздел \ref{subsection:JAGUAR_nonstoch}). Мы рассматриваем как детерминированные, так и стохастические шумы в оракуле нулевого порядка. Если первая настройка так или иначе получена в литературе \cite{lobanov2023zero, lobanov2023non}, то вторая редко рассматривается авторами, поэтому наш метод подходит для различных задач оптимизации "черного ящика".
        \item Я внедрил аппроксимацию \texttt{JAGUAR} в алгоритм Франка-Вольфа для стохастических и детерминированных задач минимизации и доказал сходимость в обоих случаях (см. разделы \ref{sect:FW_via_JAGUAR} и \ref{sect:JAGUAR_stoch}). 
        \item Я провел несколько вычислительных экспериментов, сравнивая \texttt{JAGUAR}-аппроксимацию с $l_2$-сглаживанием \eqref{eq:l2_approx} и полной аппроксимаций \eqref{eq:turtle_approx} на различных задачах минимизации (см. раздел \ref{sec:experiments}).
    \end{itemize}

\subsection{Сопутствующие работы}

    В этом разделе мы сравниваем постановки задач и методы аппроксимации в литературе о методах нулевого порядка в алгоритмах, основанных на Фрэнке-Вулфе. Некоторые авторы считают координатные методы \cite{lacoste2013block} это тоже градиентная аппроксимация, но эти методы используют истинный градиент наблюдаемой функции $f$, поэтому мы не можем напрямую применить их в оптимизации черного ящика.

    Метод $l_p$-сглаживания не требует дифференцируемости целевой функции, поскольку рассматривает сглаженную версию функции $f$ вида $f_{\gamma}(x) = \mathbb{E}_e\left[f(x + \gamma e)\right]$. В общем случае метод $l_p$-сглаживания может аппроксимировать градиент с помощью $\mathcal{O}(1)$ вызовов оракула \cite{dvinskikh2022noisy}, но он может быть не робастным в постановке Франка-Вульфа, поскольку в \cite{lobanov2023zero} авторам приходится собирать большую батч направлений $e$ для достижения сходимости. Отметим, что в \cite{lobanov2023zero} рассматривается нестохастический шум.

    Полная аппроксимация также используется в литературе \cite{sahu2019towards, gao2020can, akhtar2022zeroth}, но на каждой итерации нам необходимо делать $\mathcal{O}(d)$ вызовов оракула, а поскольку в современных приложениях $d$ огромно, это может быть проблемой. Также этот метод требует гладкости объективной функции $f$.

    В таблице \ref{tab:FW} приведено сравнение постановок задач, методов аппроксимации и результатов для них.
    
    \begin{table*}[!ht]
        \centering
        \caption{Сопоставление различных методов нулевого порядка и координатных методов ФВ.} \label{tab:FW}   
        \resizebox{\linewidth}{!}{
            \begin{tabular}{|c|c|c|c|c|c|c|}
                \hline  
                \multirow{2}{*}{Метод} & \multicolumn{2}{c|}{Постановка} & \multicolumn{2}{c|}{Шум} & \multirow{2}{*}{Размер батча} & \multirow{2}{*}{Аппроксимация} \\
                \cline{2-5}
                 & Гладкая & Нулевой порядок & Стахастический & Детерминированный & & \\
                \hline
                ZO-SCGS \cite{lobanov2023zero} & \redx & \greencheck & \redx  & \greencheck & $\mathcal{O} \left(1 / \varepsilon^2 \right)$ & $l_2$-сглаживание \eqref{eq:l2_approx} \\
                \hline
                FZFW \cite{gao2020can} & \greencheck & \greencheck & \redx & \redx & $\mathcal{O} \left( \sqrt{d} \right)$ & полная аппроксимация \eqref{eq:turtle_approx} \\
                \hline 
                DZOFW \cite{sahu2019towards} & \greencheck & \greencheck & \redx & \redx & $\mathcal{O} \left( d \right)$ & полная аппроксимация \eqref{eq:turtle_approx} \\
                \hline 
                MOST-FW \cite{akhtar2022zeroth} & \greencheck & \greencheck & \redx & \redx & $\mathcal{O} \left( d \right)$ & полная аппроксимация \eqref{eq:turtle_approx} \\
                \hline 
                BCFW \cite{lacoste2013block} & \greencheck & \redx & \redx & \redx & $\mathcal{O} \left( 1 \right)$ & координатный \\
                \hline 
                SSFW \cite{beznosikov2023sarah} & \greencheck & \redx & \redx & \redx & $\mathcal{O}\left(1\right)$ & координатный \\
                \hline 
                \rowcolor{bgcolor2} \texttt{ФВ с JAGUAR} (эта работа) & \greencheck & \greencheck & \greencheck & \greencheck & $\mathcal{O} \left( 1 \right)$ & \texttt{JAGUAR} (Алгоритмы \ref{alg:JAGUAR_nonstoch} и \ref{alg:FW_stoch}) \\
                \hline 
            \end{tabular}
    }
    \end{table*}