\section{Основные результаты}

\subsection{\texttt{JAGUAR}} \label{section:JAGUAR}
    
    Выше были рассмотрены методы аппроксимации градиента с помощью конечных разностей \eqref{eq:full_approx} и \eqref{eq:l2_approx}. В этом разделе представлены новые методы оценки градиента \texttt{JAGUAR}: \texttt{JAGUAR-d} для детерминированной и \texttt{JAGUAR-s} для стохастической задач, основанные на уже исследованных методах и использующие память предыдущих итераций.
    
    Идея метода \texttt{JAGUAR} схожа с известными методами уменьшения дисперсии, такими как SAGA \cite{defazio2014saga} или SVRG \cite{johnson2013accelerating}, но эти методы применяют уменьшение дисперсии к батчам. Однако при оптимизации нулевого порядка нужно аппроксимировать градиент, поэтому необходимо применить технику уменьшения дисперсии к координатам \cite{hanzely2018sega}. Следовательно, метод \texttt{JAGUAR} использует память некоторых координат предыдущих градиентов, а не запоминает градиенты по батчам в прошлых точках. В литературе уже есть работы, сочетающие оптимизацию нулевого порядка и уменьшение дисперсии, но суть их в том, что они меняют вычисление градиента на безградиентную аппроксимацию \eqref{eq:full_approx} в пакетных алгоритмах с уменьшением дисперсии, таких как SVRG или SPIDER \cite{ji2019improved}, а не используют технику уменьшения дисперсии для координат, как в алгоритме \ref{alg:JAGUAR_nonstoch}.  
    
    Для детерминированного алгоритма аппроксимации используется разностная схема:
    \begin{align} \label{eq:opf_nonstoch}
        \widetilde{\nabla}_i f_\delta (x) :=  \frac{f_\delta (x + \tau e_i) - f_\delta (x - \tau e_i)}{2 \tau} e_i,
    \end{align}
    где $e_i$ -- вектор из стандартного базиса в $\mathbb{R}^d$. Сам алгоритм аппроксимации градиента для детерминированной задачи \texttt{JAGUAR-d} в точке $x$ выглядит так (алгоритм \ref{alg:JAGUAR_nonstoch}):

    \begin{algorithm}[H]
        \caption{\texttt{JAGUAR-d}} \label{alg:JAGUAR_nonstoch}
        \begin{algorithmic}[1]
            \State \textbf{Вход:} $x, h \in \mathbb{R}^d$
            \State Сэмплируем $i \in \overline{1, d}$ равномерно и независимо
            \State Считаем $\widetilde{\nabla}_i f_\delta (x) = \frac{f_\delta (x + \tau e_i) - f_\delta (x - \tau e_i)}{2 \tau} e_i$
            \State $h = h - \langle h, e_i \rangle e_i + \widetilde{\nabla}_i f_\delta (x)$
            \State \textbf{Выход:} $h$
        \end{algorithmic}
    \end{algorithm}

    В стохастической постановке \eqref{eq:problem_stoch} есть две версии разностных схем \eqref{eq:opf_nonstoch}. Первая называется двухточечной обратной связью (ДОС) \cite{duchi2015optimal, shamir2017optimal, doi:10.1137/19M1259225, beznosikov2020gradient, gasnikov2022power}, в данном случае аппроксимация градиента функции $f(x, \xi)$:
    \begin{align} \label{eq:tpf}
        \widetilde{\nabla}_i f_\delta (x, \xi) :=  \frac{f_\delta (x + \tau e_i, \xi) - f_\delta (x - \tau e_i, \xi)}{2 \tau} e_i.
    \end{align}

    Вторая называется одноточечной обратной связью (ООС) \cite{nemirovskij1983problem, flaxman2004online, gasnikov2017stochastic, akhavan2020exploiting, beznosikov2021one}, в этом случае аппроксимация градиента функции $f(x, \xi)$: 
    \begin{align} \label{eq:opf}
        \widetilde{\nabla}_i f_\delta (x, \xi^{\pm}) :=  \frac{f_\delta (x + \tau e_i, \xi^+) - f_\delta (x - \tau e_i, \xi^-)}{2 \tau} e_i.
    \end{align}
    
    Ключевое различие между приближениями \eqref{eq:tpf} и \eqref{eq:opf} заключается в том, что схема \eqref{eq:tpf} более точна, но ее сложно реализовать на практике, так как для этого необходимо получить одну и ту же реализацию $\xi$ в двух разных точках $x + \tau e$ и $x - \tau e$, поэтому схема \eqref{eq:opf} более интересна с практической точки зрения. Для дальнейшего упрощения выкладок считается, что в случае двухточечной обратной связи \eqref{eq:tpf} $\xi^+ = \xi^- = \xi$. Алгоритм аппроксимации градиента для стохастической задачи \texttt{JAGUAR-s} в точке $x$ (алгоритм \ref{alg:JAGUAR_stoch}):

    \begin{algorithm}[H]
        \caption{\texttt{JAGUAR-s}} \label{alg:JAGUAR_stoch}
        \begin{algorithmic}[1]
            \State \textbf{Вход:} $x, h, g \in \mathbb{R}^d$; $\eta \in [0, 1]$
            \State Сэмплируем $i \in \overline{1, d}$ равномерно и независимо
            \State Сэмплируем $\xi$: $\xi^+$ и $\xi^-$ независимо (в ДОС $\xi^+ = \xi^-$)
            \State Считаем $\widetilde{\nabla}_i f_\delta (x, \xi^\pm) = \frac{f_\delta (x + \tau e_i, \xi^+) - f_\delta (x - \tau e_i, \xi^-)}{2 \tau} e_i$
            \State $h = h - \langle h, e_i \rangle e_i + \widetilde{\nabla}_i f_\delta (x, \xi^+, \xi^-)$
            \State $\rho = h - d \cdot \langle h, e_i \rangle e_i + d \cdot \widetilde{\nabla}_i f_\delta (x, \xi^+, \xi^-)$
            \State $g = (1 - \eta) g + \eta \rho$ 
            \State \textbf{Выход:} $g, h$
        \end{algorithmic}
    \end{algorithm}

    Алгоритм \texttt{JAGUAR-s} (алгоритм \ref{alg:JAGUAR_stoch}) аналогичен \texttt{JAGUAR-d} (алгоритм \ref{alg:JAGUAR_nonstoch}), но в строках 6 и 7 используются части SEGA \cite{hanzely2018sega} и моментум \cite{mokhtari2020stochastic} для сходимости в стохастическом случае.

    В стохастическом случае необходима часть SEGA \cite{hanzely2018sega} $\rho_k$ в алгоритме \ref{alg:JAGUAR_stoch}, поскольку важно свойство <<несмещенности>> (см. доказательство леммы \ref{lemma:g_vs_nabla_f}), т.е.
    \begin{align*}
       \mathbb{E}_k \left[ \rho^k \right] = \widetilde{\nabla} f_\delta (x^k) := \sum\limits_{i = 1}^{d} \frac{f_\delta (x + \tau e_i) - f_\delta(x - \tau e_i)}{2 \tau} e_i, 
    \end{align*}
    где $\mathbb{E}_k \left[ \rho^k \right]$ -- условное математическое ожидание на шаге $k$. Использование части SEGA $\rho^k$ ухудшает оценки в $d$ раз по сравнению с использованием $h^k$ в качестве градиентной аппроксимации (см. леммы \ref{lemma:h_vs_nablaf} и \ref{lemma:rho_vs_nablaf}).

    В стохастическом случае необходим моментум \cite{mokhtari2020stochastic} $\eta_k$ в алгоритме \ref{alg:JAGUAR_stoch}, поскольку при оценке выражения $\mathbb{E} \left[ \left\| \widetilde{\nabla} f_\delta (x, \xi_{\overline{1, d}}^\pm) - \nabla f(x) \right\|^2 \right]$ в стохастическом случае появляются выражения, содержащие $\sigma_\nabla^2$ и $\sigma_f^2$, и они мешают сходимости (см. лемму \ref{lemma:tilde_vs_notilda}).

\subsubsection{Применение \texttt{JAGUAR}}

    Алгоритм аппроксимации градиента \texttt{JAGUAR-d} может быть использован с любыми итерационными схемами, которые на каждом шаге $k$ возвращают новую точку $x^k$. Используя эти точки, мы получается последовательность $h^k$, которая в некотором смысле служит памятью компонент градиента из прошлых моментов. Поэтому в методах инкрементальной оптимизации имеет смысл использовать $h^k$ в качестве оценки истинного градиента $\nabla f(x^k)$. Используя следующую унифицированную схему, можно описать такой итерационный алгоритм, решающий задачу \eqref{eq:problem_nonstoch} (алгоритм \ref{alg:iter_nonstoch}).
        
    \begin{algorithm}[H]
        \caption{Итерационный алгоритм с использованием \texttt{JAGUAR-d}} \label{alg:iter_nonstoch}
        \begin{algorithmic}[1]
            \State \textbf{Вход:} \texttt{Proc} и $h^0$
        	\For {$k = 0, 1, 2, ... , N$}
                \State $h^{k + 1} = \texttt{JAGUAR-d}(x^k, h^k)$
                \State $x^{k + 1} = \texttt{Proc}(x^k, \texttt{grad\_est} = h^{k + 1})$
            \EndFor
            \State \textbf{Выход:} $x^{N + 1}$
        \end{algorithmic}
    \end{algorithm}

    Используя алгоритм аппроксимации градиента \texttt{JAGUAR-s}, можно описать такой итерационный алгоритм, решающий задачу \eqref{eq:problem_nonstoch} + \eqref{eq:problem_stoch} (алгоритм \ref{alg:iter_stoch}).

    \begin{algorithm}[H]
        \caption{Итерационный алгоритм с использованием \texttt{JAGUAR-s}} \label{alg:iter_stoch}
        \begin{algorithmic}[1]
            \State \textbf{Вход:} \texttt{Proc} и $h^0$
        	\For {$k = 0, 1, 2, ... , N$}
                \State $h^{k + 1}, g^{k + 1} = \texttt{JAGUAR-s}(x^k, h^k, g^k, \eta_k)$
                \State $x^{k + 1} = \texttt{Proc}(x^k, \texttt{grad\_est} = g^{k + 1})$
            \EndFor
            \State \textbf{Выход:} $x^{N + 1}$
        \end{algorithmic}
    \end{algorithm}

    В алгоритмах \ref{alg:iter_nonstoch} и \ref{alg:iter_stoch}, \texttt{Proc}($x^k$, \texttt{grad\_est}) -- это некоторая последовательность действий, которая переводит $x^k$ в $x^{k + 1}$, используя \texttt{grad\_est} в качестве истинного градиента. Ниже приведен анализ аппроксимации градиента \texttt{JAGUAR}. Необходимо оценить близость истинного градиента $\nabla f(x^k)$ / $\nabla f(x^k, \xi)$ и выхода алгоритма \texttt{JAGUAR} $h^k$ / $g^k$ на шаге $k$. 
    
\subsubsection{Анализ аппроксимаций \texttt{JAGUAR}}

    Для анализа алгоритма \ref{alg:iter_nonstoch} используется обозначение:
    \begin{align*}
        \widetilde{\nabla} f_\delta (x) := \sum\limits_{i = 1}^d \frac{f_\delta (x + \tau e_i) - f_\delta (x - \tau e_i)}{2 \tau} e_i. 
    \end{align*}

    Для анализа алгоритма \ref{alg:iter_stoch} используется обозначение:
    \begin{align*}
        \widetilde{\nabla} f_\delta (x, \xi_{\overline{1, d}}^\pm) := \sum\limits_{i = 1}^d \frac{f_\delta (x + \tau e_i, \xi_i^+) - f_\delta (x - \tau e_i, \xi_i^-)}{2 \tau} e_i.
    \end{align*}

    Для упрощения выкладок в случае двухточечной обратной связи $\sigma_f = 0$, а в детерминированном случае \eqref{eq:problem_nonstoch} $\sigma_\nabla = \sigma_f = 0$.
    
    \begin{lemma} \label{lemma:tilde_vs_notilda}
    
        При предположениях \ref{ass:smooth_stoch}, \ref{ass:bounded_stoch}, \ref{ass:sigma_nabla} и \ref{ass:sigma_f} в случае ООС \eqref{eq:opf} выполняется следующее неравенство:
        \begin{align*}
            \mathbb{E} \left[ \left\| \widetilde{\nabla}f_{\delta}(x, \xi^+_1, \xi^-_1, ... , \xi_d^+, \xi_d^-) - \nabla f(x) \right\|^2 \right] \leq d L^2 \tau^2 + \frac{8 d \sigma_f^2}{\tau^2} + 2 d \sigma_{\nabla}^2 + \frac{2 d \Delta^2}{\tau^2}.
        \end{align*}
        
    \end{lemma}

    \begin{lemma} \label{lemma:h_vs_nablaf}
    
        При предположениях \ref{ass:smooth_stoch}, \ref{ass:bounded_stoch}, \ref{ass:sigma_nabla} и \ref{ass:sigma_f} в случае ООС \eqref{eq:opf} выполняется следующее неравенство:
        \begin{align*}
            \mathbb{E} \left[ \left\| h^k - \nabla f(x^k) \right\|^2 \right]
            \leq &
            \left( 1 - \frac{1}{2 d} \right) \mathbb{E} \left[ \left\| h^{k - 1} - \nabla f(x^{k - 1}) \right\|^2 \right] \\
            &+
            2d L^2 \mathbb{E} \left[ \left\| x^k - x^{k - 1} \right\|^2 \right] \\
            &+
            L^2 \tau^2 + \frac{8 \sigma_f^2}{\tau^2} + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2}.
        \end{align*}
        
    \end{lemma}

    \begin{lemma} \label{lemma:rho_vs_nablaf}
    
        При предположениях \ref{ass:smooth_stoch}, \ref{ass:bounded_stoch}, \ref{ass:sigma_nabla} и \ref{ass:sigma_f} в случае ООС \eqref{eq:opf} выполняется следующее неравенство:
        \begin{align*}
            \mathbb{E} \left[ \left\| \rho^k - \nabla f(x^k) \right\|^2 \right]
            \leq &
            4d \mathbb{E} \left[ \left\| h^{k - 1} - \nabla f(x^{k - 1}) \right\|^2 \right] \\
            &+
            2d L^2 \mathbb{E} \left[ \left\|x^k - x^{k - 1} \right\|^2 \right] \\
            &+
            4d^2 \left( L^2 \tau^2 + \frac{8 \sigma_f^2}{\tau^2} + 2 \sigma_{\nabla}^2 + \frac{2 \Delta^2}{\tau^2} \right).
        \end{align*}
        
        В случае двухточечной обратной связи $\sigma_f = 0$.
        
    \end{lemma}

    \begin{lemma} \label{lemma:g_vs_nabla_f}
    
        При предположении \ref{ass:smooth_stoch} выполняется следующее неравенство:
        \begin{align*}
            \mathbb{E} \left[ \left\| g^k - \nabla f(x^k) \right\|^2 \right]
            \leq &
            \left(1 - \eta_k \right)^2 \mathbb{E} \left[ \left\| \nabla f(x^{k - 1}) - g^{k - 1} \right\|^2 \right] \\
            &+
            \frac{4 L^2}{\eta_k} \mathbb{E} \left[ \left\| x^k - x^{k - 1} \right\|^2 \right] \\
            &+
            \eta_k^2 \mathbb{E} \left[ \left\| \nabla f(x^k) - \rho^k \right\|^2 \right] \\
            &+
            3 \eta_k \left( d L^2 \tau^2 + \frac{2 d \Delta^2}{\tau^2} \right).
        \end{align*}
        
    \end{lemma}

\subsection{Применение \texttt{JAGUAR} в алгоритме Франка-Вульфа}

    Обычный алгоритм Франка-Вульфа выглядит следующим образом (алгоритм \ref{alg:FW}):
    
    \begin{algorithm}[H]
        \caption{Алгоритм Франка-Вульфа} \label{alg:FW}
        \begin{algorithmic}[1]
            \State \textbf{Вход:} $x_0 \in Q$, $\gamma_k$
        	\For {k = 0, 1, 2, ... , N}
                \State $s^k = \argmin\limits_{s \in Q} \left< s, \nabla f(x^k) \right>$
                \State $x^{k+1} = x^k + \gamma_k (s^k - x^k)$
            \EndFor
            \State \textbf{Выход:} $x^{N + 1}$ 
        \end{algorithmic}
    \end{algorithm}

    На множество $Q$ накладываются общие ограничения:
    \begin{itemize}
        \item Множество $Q$ -- компактное, т.е.
        \begin{align} \label{ass:compact}
            \exists D > 0 : \forall x, y \in Q \hookrightarrow \|x - y\| \leq D
        \end{align}
        \item Множество $Q$ -- выпуклое, т.е.
        \begin{align} \label{ass:convex}
            \forall 0 \leq \alpha \leq 1, \forall x, y \in Q \hookrightarrow \alpha x + (1 - \alpha) y \in Q
        \end{align}
    \end{itemize}

    В следующих разделах рассмотрены детерминированные и стохастические алгоритмы Франка-Вульфа с использованием аппроксимации градиента \texttt{JAGUAR}.

\subsubsection{Детерминированный случай} \label{section:FW_nonstoch}

    В этом разделе представляется алгоритм Франка-Вульфа, который решает задачу \eqref{eq:problem_nonstoch} с помощью аппроксимации градиента \texttt{JAGUAR} (алгоритм \ref{alg:JAGUAR_nonstoch}).

    \begin{algorithm}[H]
        \caption{Детерминированный алгоритм Франка-Вульфа с \texttt{JAGUAR}} \label{alg:FW_nonstoch}
        \begin{algorithmic}[1]
        	\State \textbf{Вход:} $x^0 \in Q$, $h^0 = \widetilde{\nabla} f_\delta (x^0)$, $\gamma_k$, $\tau$
        	\For {$k = 0, 1, 2, ... , N$}
                \State $h^{k + 1}$ = \texttt{JAGUAR-d} $\left( x^k, h^k \right)$
                \State $s^k = \argmin\limits_{x \in Q} \left< s, h^{k + 1} \right>$
                \State $x^{k + 1} = x^k + \gamma_k (s^k - x^k)$
            \EndFor
            \State \textbf{Вход:} $x^{N + 1}$
        \end{algorithmic}
    \end{algorithm}

    Используя заданную форму функции \texttt{Proc} в алгоритме \ref{alg:FW_nonstoch}, можно тщательно подобрать шаг $\gamma_k$.

    \begin{theorem}[Богданов А., Подбор шага для детерминированного алгоритма Франка-Вульфа с \texttt{JAGUAR}] \label{theorem:JAGUAR_nonstoch}
    
        В предположениях \ref{ass:compact}, \ref{ass:convex} и \ref{ass:smooth_nonstoch}, \ref{ass:conv_nonstoch} и \ref{ass:bounded_nonstoch} для $h^k$, полученного алгоритмом \ref{alg:FW_nonstoch}, можно взять
        \begin{align*}
            \gamma_k = \frac{4}{k + 8d},
        \end{align*}
        тогда выполняется следующая оценка:
        \begin{align*}
            H_k = \mathcal{O} \left( \left\| \widetilde{\nabla}f_{\delta}(x^k) - \nabla f(x^k) \right\|^2 + \frac{d^2 \max\{L^2 D^2, H_0\}}{(k + d)^2} \right).
        \end{align*}
    
        \noindent Если дополнительно $h^0 = \widetilde{\nabla} f_\delta (x^0) = \sum\limits_{i = 1}^d \frac{f_\delta (x^0 + \tau e_i) - f_\delta (x^0 - \tau e_i)}{2 \tau} e_i$, можно получить:
        \begin{align*}
            H_k = \mathcal{O} \left( \left\| \widetilde{\nabla}f_{\delta}(x^k) - \nabla f(x^k) \right\|^2 +\frac{d^2 L^2 D^2}{(k + 8d)^2} \right),
        \end{align*}
        где используется обозначение $H_k := \mathbb{E} \left[ \left\| h^k - \nabla f(x^k) \right\|^2 \right]$.
        
        \noindent Подробное доказательство теоремы приведено в Приложении \ref{appendix:FW}.
        
    \end{theorem}

    Из Теоремы \ref{theorem:JAGUAR_nonstoch} следует, что после $\mathcal{O} \left( \frac{\sqrt{d} D}{\tau} \right)$ шагов получается такая же оценка, что и в полной аппроксимации.

    \begin{theorem}[Богданов А., Скорость сходимости детерминированного алгоритма Франка-Вульфа с \texttt{JAGUAR} (Алгоритм \ref{alg:FW_nonstoch})] \label{theorem:FW_nonstoch}
        
        В предположениях \ref{ass:compact}, \ref{ass:convex}, \ref{ass:smooth_nonstoch}, \ref{ass:conv_nonstoch} и \ref{ass:bounded_nonstoch} можно взять
        \begin{align*}
            \gamma_k = \frac{4}{k + 8d},
        \end{align*}
        тогда алгоритм Франка-Вульфа с \texttt{JAGUAR} (алгоритм \ref{alg:FW_nonstoch}) имеет следующую скорость сходимости:
        \begin{align*}
            \mathbb{E} \left[ f(x^k) - f^* \right] = \mathcal{O} \left( \frac{d \max \{L D^2, F_0 \}}{N + 8d} + \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau}\right).
        \end{align*}

        \noindent Подробное доказательство теоремы приведено в Приложении \ref{appendix:FW}.
        
    \end{theorem}

    \begin{corollary} \label{cor:FW_nonstoch}
    
        В соответствии с условиями теоремы \ref{theorem:FW_nonstoch}, выбирая $\gamma_k, \tau, \Delta$ как
        \begin{align*}
            \gamma_k = \frac{4}{k + 8d},\ \tau = \mathcal{O} \left( \frac{\varepsilon}{\sqrt{d} L D} \right),\ \Delta = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2} \right),
        \end{align*}
        чтобы получить $\varepsilon$-приближенное решение $(\mathbb{E} \left[f(x^k) - f^* \right] \leq \varepsilon)$ необходимо
        \begin{align*}
            \mathcal{O} \left( \frac{d \max \{ L D^2, F_0 \}}{\varepsilon} \right)\ \text{итераций}.
        \end{align*}

        \noindent Подробное доказательство следствия приведено в Приложении \ref{appendix:FW}.
        
    \end{corollary}

    Результаты Теоремы \ref{theorem:FW_nonstoch} совпадают с результатами \cite{frank1956algorithm, jaggi2013revisiting}, в которых авторы использовали истинный градиент и получили результат вида $\mathbb{E} \left[ f (x^N) - f^* \right] = \mathcal{O} \left( \max \{ LD^2; f(x^0) - f^* \} / N \right)$. В случае нулевого порядка неизбежно появляются члены вида $\mathcal{O} \left( \text{poly} (\tau) + \text{poly} (\Delta / \tau) \right)$, поскольку они имеют решающее значение для аппроксимации истинного градиента и всегда влияют на сходимость методов нулевого порядка \cite{risteski2016algorithms, sahu2018distributed, liu2018zeroth, beznosikov2020derivative}. Фактор $d$, который появляется в теоретических оценках по сравнению с результатом первого порядка, связан со структурой метода нулевого порядка.

\subsubsection{Стохастический случай} \label{section:JAGUAR_stoch}

    В этом разделе рассматривается алгоритм Франка-Вульфа, который решает задачу \eqref{eq:problem_nonstoch} + \eqref{eq:problem_stoch} с помощью аппроксимации градиента \texttt{JAGUAR} (алгоритм \ref{alg:JAGUAR_stoch}).

    \begin{algorithm}[H]
        \caption{Стохастический алгоритм Франка-Вульфа с \texttt{JAGUAR}} \label{alg:FW_stoch}
        \begin{algorithmic}[1]
        	\State \textbf{Вход:} $x^0 \in Q$, $h^0 = g^0 = \widetilde{\nabla} f_\delta (x^0)$, $\gamma_k$, $\eta_k$, $\tau$
        	\For {$k = 0, 1, 2, ... , N$}
                \State $g^{k + 1}$, $h^{k + 1}$ = \texttt{JAGUAR-s} $\left( x^k, h^k, g^k, \eta_k \right)$
                \State $s^k = \argmin\limits_{x \in Q} \left< s, g^{k + 1} \right>$
                \State $x^{k + 1} = x^k + \gamma_k (s^k - x^k)$
            \EndFor
            \State \textbf{Вход:} $x^{N + 1}$
        \end{algorithmic}
    \end{algorithm}

    Можно получить теорему, аналогичную теореме \ref{theorem:JAGUAR_nonstoch}, если тщательно подобрать размеры шагов $\gamma_k$ и $\eta_k$. 
    
    \begin{theorem}[Богданов А., Подбор шага для стохастического алгоритма Франка-Вульфа с \texttt{JAGUAR}] \label{theorem:JAGUAR}
    
        В предположениях \ref{ass:compact}, \ref{ass:convex}, \ref{ass:smooth_stoch}, \ref{ass:conv_stoch}, \ref{ass:bounded_stoch}, \ref{ass:sigma_nabla} и \ref{ass:sigma_f} в случае одноточечной обратной связи, для $g^k$, полученного алгоритмом \ref{alg:FW_stoch}, можно взять
        \begin{align*}
            \gamma_k = \frac{4}{k + 8d^{3/2}} \quad \text{и} \quad \eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}},
        \end{align*}
        тогда выполняется следующая оценка:
        \begin{align*}
            G_k = \mathcal{O}
            &\Bigg(
            \frac{L^2 D^2 + \max \{ d^2 \sigma_f^2 / \tau^2 + d^2 \sigma_\nabla^2; d G_0\}}{(k + 8d^{3 / 2})^{2 / 3}}\\
            &+
            \frac{d^4 \left\| h^0 - \nabla f(x^0) \right\|^2}{(k + 8d^{3 / 2})^{8 / 3}} + d L^2 \tau^2 + \frac{d \Delta^2}{\tau^2} \Bigg).
        \end{align*}

        \noindent Если дополнительно $h^0 = g^0 = \widetilde{\nabla} f_\delta (x^0, \xi_{\overline{1, d}}^\pm)$, то получается:
        \begin{align*}
            G_k = \mathcal{O} \left( \left\| \widetilde{\nabla} f_\delta (x^k) - \nabla f(x^k) \right\|^2 + \frac{L^2 D^2 + d^2 \sigma_f^2 / \tau^2 + d^2 \sigma_\nabla^2}{(k + 8d^{3 / 2})^{2 / 3}} \right),
        \end{align*}
        где используется обозначение $G_k := \mathbb{E} \left[ \left\| g^k - \nabla f(x^k) \right\|^2 \right]$. В случае двухточечной обратной связи $\sigma^2_f = 0$.

        \noindent Подробное доказательство теоремы приведено в Приложении \ref{appendix:JAGUAR}.
        
    \end{theorem}
    
    Полученная оценка хуже по сравнению с детерминированным случаем в Теореме \ref{theorem:JAGUAR_nonstoch}, поскольку рассматривается более сложная постановка.

    \begin{theorem}[Богданов А., Скорость сходимости стохастического алгоритма Франка-Вульфа с \texttt{JAGUAR} (Алгоритм \ref{alg:FW_stoch})] \label{theorem:FW}
    
        В предположених \ref{ass:compact}, \ref{ass:convex}, \ref{ass:smooth_stoch}, \ref{ass:conv_stoch} \ref{ass:bounded_stoch}, \ref{ass:sigma_nabla} и \ref{ass:sigma_f} в случае одноточечной обратной связи можно взять: 
        \begin{align*}
           \gamma_k = \frac{4}{k + 8d^{3/2}} \quad \text{и} \quad \eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}},
        \end{align*}
        тогда алгоритм Франка-Вульфа с \texttt{JAGUAR} (Алгоритм \ref{alg:FW_stoch}) имеет следующую скорость сходимости:
        \begin{align*}
            F_N = \mathcal{O} \left( \frac{L D^2 + d \sigma_f D/ \tau + d \sigma_\nabla D + \sqrt{d} F_0}{(N + 8d^{3/2})^{1/3}} + \sqrt{d} L D \tau + \frac{\sqrt{d} \Delta D}{\tau} \right),
        \end{align*}
        где используется обозначение $F_k := \mathbb{E} \left[ f(x^{k}) - f^* \right]$. В случае двухточечной обратной связи $\sigma_f = 0$.

        \noindent Подробное доказательство теоремы приведено в Приложении \ref{appendix:FW}.
            
    \end{theorem}

    \begin{corollary} \label{cor:FW}
        В соответствии с условиями теоремы \ref{theorem:FW}, выбирая $\gamma_k, \eta_k, \tau, \Delta$ как
        \begin{align*}
            \gamma_k = \frac{4}{k + 8d^{3/2}},\ \eta_k = \frac{4}{(k + 8d^{3/2})^{2/3}},\ \tau = \mathcal{O} \left(\frac{\varepsilon}{\sqrt{d} L D} \right),\ \Delta = \mathcal{O} \left( \frac{\varepsilon^2}{d L D^2}\right),
        \end{align*}
        чтобы получить $\varepsilon$-приближенное решение $(\mathbb{E} \left[ f(x^N) - f^* \right] \leq \varepsilon)$ необходимо
        \begin{align*}
            \mathcal{O} \left( \max \left\{ \left[ \frac{L D^2 + d \sigma_\nabla D + \sqrt{d} (f(x^0) - f^*)}{\varepsilon} \right]^3; \frac{d^{9 / 2} \sigma_f^3 L^3 D^6}{\varepsilon^6} \right\} \right)\ \text{итераций}.
            \end{align*}

        \noindent В случае двухточечной обратной связи $\sigma_f = 0$ и последнее выражение принимает вид
        \begin{align*}
            \mathcal{O} \left( \left[ \frac{L D^2 + d \sigma_\nabla D + \sqrt{d} (f(x^0) - f^*)}{\varepsilon} \right]^3 \right)\ \text{итераций}.
        \end{align*}

        \noindent Подробное доказательство следствия приведено в Приложении \ref{appendix:FW}.
            
    \end{corollary}

    Поскольку в алгоритме аппроксимации \texttt{JAGUAR} (алгоритм \ref{alg:FW_stoch}) использовались части SEGA и импульса, то не получается та же скорость сходимости, что и в теоремах \ref{theorem:JAGUAR_nonstoch} и \ref{theorem:FW_nonstoch} даже при переходе от стохастических к детерминированным настройкам, т.е, при задании $\sigma_\Delta = \sigma_f = 0$ в теоремах \ref{theorem:JAGUAR} и \ref{theorem:FW}. Те же проблемы возникают и в случае первого порядка \cite{mokhtari2020stochastic, zhang2020one}, это связано с трудностями реализации стохастического градиента в алгоритмах типа Франка-Вульфа. 

    Можно применить \texttt{JAGUAR-d} (алгоритм \ref{alg:JAGUAR_nonstoch}) к стохастической задаче \eqref{eq:problem_stoch} и получить те же оценки, что и в Теоремах \ref{theorem:JAGUAR_nonstoch} и \ref{theorem:FW_nonstoch}, только сглаженный член вида $\mathcal{O} \left( \text{poly} (\tau) + \text{poly} (\Delta / \tau) \right)$ будет содержать слагаемые вида $\mathcal{O} \left( \text{poly}(\sigma_\Delta^2) + \text{poly} (\sigma_f^2 / \tau) \right)$. Поэтому, если $\sigma_\Delta^2, \sigma_f^2 \sim \Delta$, то детерминированный алгоритм \ref{alg:JAGUAR_nonstoch} подходит для стохастической задачи \eqref{eq:problem_stoch}. Однако это означает, что нужно использовать большие батчи, поэтому необходимо использовать SEGA и импульсные части в \texttt{JAGUAR-s} аппроксимации.